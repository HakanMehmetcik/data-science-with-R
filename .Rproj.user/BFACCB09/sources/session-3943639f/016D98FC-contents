---
title: "Week1: Basics"
author: "Hakan Mehmetcik"
format: pdf
editor: visual
execute: 
  echo: true
  warning: true
  output: asis
df-print: kable
---

```{r setup, echo=FALSE, message=FALSE}

# loaded packages
library(here)
library(tidyverse)
library(ggplot2)
library(mdsr)

here <- here::here()
dpath <- "data"
```

## **Collective Statistical Illiteracy: Understanding the Challenge**

In contemporary society, almost every news article or broadcast includes some form of scientific or numerical data. Headlines routinely mention statistics about public health, economic indicators, environmental changes, and more. However, a significant portion of the audience struggles to interpret these numbers and to understand how they were derived. This difficulty can have profound implications for personal decision-making, organizational strategy, and public policy.

The concept of **collective statistical illiteracy** highlights this widespread inability to grasp the meaning behind statistical facts and figures. When individuals lack the skills to critically evaluate quantitative information, they are more susceptible to misunderstanding important issues, making poorly informed decisions, or being swayed by misleading statistics.

> ***Statistical** thinking will one day be as necessary for efficient citizenship as the ability to read and write.*
>
> H.G. Wells (1903, paraphrased by S.S. Wilks, see [link](http://quotablemath.blogspot.com/2018/02/misquoted-hg-wells-on-statistics.html))

## **Why Study Data Science?**

Data science drives innovation and decision-making in virtually every industryâ€”from healthcare and finance to social media and marketing. By combining computational methods with statistical analysis, data science uncovers hidden patterns and insights that help organizations craft evidence-based strategies, predict future trends, and make more informed decisions. As the volume of data grows exponentially, the ability to collect, process, and interpret this information becomes ever more critical.

### **How Is Data Science Different from Math and Statistics?**

-   **Interdisciplinary Approach**: While mathematics and statistics focus on theoretical models and the science of uncertainty, data science integrates computer programming, domain knowledge, and data engineering alongside statistical techniques.

-   **Practical, Data-Driven Focus**: Data scientists often work with real-world data that may be messy or unstructured. They prioritize building reproducible workflows and scalable analyses, which is not the central concern in pure math or traditional statistics.

-   **Technology and Tools**: Data science heavily relies on programming languages (e.g., Python, R) and tools (e.g., machine learning frameworks, data visualization platforms) to handle large, complex datasetsâ€”capabilities not typically emphasized in standard math or statistics coursework.

*ğŸš€ **Data science***---the science of extracting meaningful information from data. As such, data science is a broader field than statistics, encompassing data gathering, preparation, modeling, visualization, computing, and meta-analysis of data science itself.

::: callout-note
An assumption underlying these efforts is that data is the foundation of various information types that can eventually be turned into knowledge and wisdom. Figure below shows their arrangement in a hierarchical structure of a [DIKW pyramid](https://en.wikipedia.org/wiki/DIKW_pyramid):

![](images/DIKW_Pyramid.svg.png)

The key distinction between the lower and the upper layers of the pyramid is that data needs to address some hypothesis or answer some question, and has to be interpreted and understood to become valuable. Here to say, another reason for the close interaction between data and theory is that we need theoretical models for understanding and interpreting data. When analyzing data, we are typically interested in the underlying mechanisms (i.e., the causal relationships between variables). Importantly, any pattern of data can be useless and misleading, when the data-generating process is unknown or ignored. Knowledge or at least assumptions regarding the causal process illuminate the data and are required for its sound interpretation. The importance of theoretical assumptions for data analysis cannot be underestimated (see, e.g., the notion of causal models and counterfactual reasoning in [Pearl & Mackenzie, 2018](https://bookdown.org/hneth/i2ds/references.html#ref-PearlMackenzie18)). Thus, **pitting data against theory is nonsense**: Using and understanding data is always based on theory.
:::

Our aims is to prepare **well-documented and reproducible** analysis since data literacy is the ability and skill of making sense of data. This includes numeracy, risk-literacy, and the ability of using tools to collect, transform, analyze, interpret, and present data, in a transparent, reproducible, and responsible fashion.

ğŸ¤” **Information** is what we want, but data are what weâ€™ve got. The techniques for transforming data into information is the ***data science!***

**Data scientists, therefore, are individuals who strive to transform the plentiful data available today into actionable information, which often appears to be in short supply**

ğŸš€ ***Think with data = desire to solve problems using data***

ğŸš€ **A data scientist** is "a knowledge worker" who is principally occupied with analyzing complex and massive data resources.

ğŸš€**Demand** for data skills is strong! Data Scientist is one of the best job in the world since 2016.

ğŸš€The Key components that are part of [***data acumen***]{.underline} include mathematical, computational, and statistical foundations, data management and curation, data description and visualization, data modeling and assessment, workflow and reproducibility, communication and teamwork, domain-specific considerations, and ethical problem solving.Â 

ğŸš€Therefore, contemporary data science requires **tight integration of statistical, computational, and communication skills.**

ğŸš€**Data Wrangling** a process of preparing data for visualization and other modern techniuqes of statistical interpretations.

::: callout-note
**Recommended Background Readings**

1.  **Baumer, B. S., Kaplan, D. T., & Horton, N. J. (2021). *Modern Data Science with R*.**

    -   **Chapter 1: Prologue: Why Data Science?**

    -   This chapter introduces the motivation behind data science as a discipline. It discusses how the field has evolved, its interdisciplinary nature, and why it is relevant today. The reading helps students understand the **philosophical and practical reasons for studying data science**.

2.  **Donoho, D. (2017). "50 Years of Data Science." *Journal of Computational and Graphical Statistics*, 26(4), 745-766.**

    -   A historical perspective on the evolution of **data science as a field**, emphasizing how it extends beyond traditional statistics.

    -   Discusses the distinction between **data science and statistics**, highlighting the importance of computational tools.

    -   Advocates for a more **inclusive and interdisciplinary** approach to data science education.

3.  **De Veaux, R. D., Agarwal, M., Averett, M., Baumer, B. S., Bray, A., Bressoud, T. C., & others. (2017). "Curriculum Guidelines for Undergraduate Programs in Data Science." *Annual Review of Statistics and Its Application*, 4, 15-30.**

    -   Provides an overview of the **core competencies needed for data science education**, including programming, statistical inference, and domain expertise.

    -   Defines **guidelines for structuring an undergraduate data science curriculum**, which is useful for understanding how your course fits within broader educational frameworks.

    -   Emphasizes the importance of **ethics, reproducibility, and communication skills** in data science training.
:::

## What is data?

Today, the manner in which we extract meaning from data is different in two ways---both due primarily to advances in computing:

-   we are able to compute many more things than we could before, and,

-   we have a *lot* more data than we had before.

ğŸš€ **the traditional two-dimensional representation of data:** rows and columns in a data table, and horizontal and vertical in a data graphic. For instance, if someone aimed to collect or compare health-related characteristics of some people, he or she would measure and record these characteristics in some file. In such a file of health records, some values may identify persons (e.g., by name or some IDÂ code), while others describe them by numbers (e.g., their age, height, etc.) or various codes or text labels (e.g., their address, profession, diagnosis, notes on allergies, medication, vaccinations, etc.). The variables and values are typically stored in tabular form: If each of the tableâ€™s rows describes an individual person as our unit of observation, its columns are called *variables* and the entries in the cells (which can be referenced as combinations of rows and columns) are *values*. The contents of the entire table (i.e., the observations, variables, and values) are typically called â€œdataâ€.

**Non-traditional data types (e.g., geospatial, text, network, "big") and interactive data graphic** are the new normal! Thus, we can argue that the gap between generating scientific insights and understanding them is widening.

The increasing complexity and heterogeneity of modern data means that each data analysis project needs to be custom-built. Simply put, the modern data analyst needs to be able to read and write computer instructions, the â€œcodeâ€ from which data analysis projects are built.

ğŸš€ **Domain knowledge**Â is always useful in data science since data science is best applied in the context of expert knowledge about the domain from which the data originate. For data scientists of all application domains, creativity, domain knowledge, and technical ability are absolutely essential.

Some related discussions in this contexts include:

1.  The distinction between *raw data* (e.g., measurements, inputs) and *processed data* (results or interpretations): Data are often portrayed as a potential resource: Something that can be collected or generated, harvested, and â€”Â by processingÂ â€” refined or distilled to turn it into something more valuable. The related term *data mining* also suggests that data is some passive raw material, whereas the active processing and interpretation of data (by algorithms, rules, instructions) can generate interest and revenue.

2.  The distinction between *data* and *information*: In contrast to data, the term *information* has a clear formal definition (see [Wikipedia: Information theory](https://en.wikipedia.org/wiki/Information_theory)). And while data appears to be neutral, information is usually viewed as something positive and valuable. Hence, when data is being used and perceived as useful, we can generate *insight*, *knowledge*, or perhaps even *wisdom* from it?

3.  The field of signal detection theory (SDT, see [Wikipedia: Detection theory](https://en.wikipedia.org/wiki/Detection_theory)) distinguishes between *signal* and *noise*. Does *data* include both signal and noise, or should we only count the signal parts as data?

## What is Data Science (DS)?

DS is not a single and homogeneous discipline. Instead, it overlaps and is intricately interwoven with several other academic fields and requires corresponding skills. For instance, becoming an expert in DS requires considerable knowledge in statistics, but the discipline of data science is not as mathematical as statistics. Similarly, data science involves computers, but is usually not as much concerned with abstract formalisms as computer science. We could describe DS as applied statistics or applied computer science, but this would suggest that it is a sub-discipline of these other fields â€”Â and any serious application ususally assumes knowledge of the disciplineâ€™s foundations.

![](images/ds.png)

AsÂ R is defined as a â€œsoftware environment for statistical computing and graphicsâ€ ([R Core Team, 2023](https://bookdown.org/hneth/i2ds/references.html#ref-R)), it supports and provides tools for all topics mentioned and is thus pictured at the center. However, this central position does not imply that R is the only or a necessary tool for DS (e.g., a popular alternative is the programming language [Python](https://www.python.org/)).

::: callout-note
ğŸ’¡ A striking example of bad software choices is the Public Health England (PHE)â€™s recent decision to import CSV-files on COVID-19 test results into Microsoft Excelâ€™s XLS file format. Due to this file formatâ€™s artificial limit to a maximum of 65.536Â rows of data, nearly 16.000Â Covid cases went unreported in the U.K. (which amounts to almost 24% of the cases recorded in the time span from SepÂ 25 to Oct.Â 2, 2020).
:::

The skills for successfully dealing with data are not confined to one discipline or talent. As data scientists must discover, mine, select, organize, transform, analyze, understand, communicate and present information, they tend to be generalists, rather than specialists. Beyond a set of skills from a diverse range of areas, getting DS done requires the familiarity with and mastery of suitable tools.

## The *same* data can be represented in many *different types* and *shapes*.

#### **Types of data**

As variables and values depend on what we want to measure (i.e., our goals), it is impossible to provide a comprehensive list of data types. Nevertheless, computer scientists typically categorize data into different types. This is made possible by distinguishing between different ways in which data is represented. The three most basic types of data are:

1.  *Truth values* (aka. *logicals*): either `TRUE` or `FALSE`

2.  *Numbers*: e.g., 2,12,âˆš22,12,2

3.  *Text* (aka. *characters* or *strings*): e.g., â€œDonald Duckâ€ or â€œWar and Peaceâ€

Most computer languages have ways to represent these three elementary types. Exactly how truth values, numbers, or text relate to the actual phenomena being described involves many issues of representation and measurement.

In addition to these three basic data types, there are common data types that have precise and widely-shared definitions, like

-   *dates* and *times*: e.g., 2025-02-10, `11:55`

-   *locations*: e.g., the summit of Mount Everest, or N40âˆ˜âˆ˜ 44.9064â€™, W073âˆ˜âˆ˜ 59.0735â€™

but even more potential data types that we could define, if we had or wanted to, for instance

-   *temporal terms*: e.g., `Monday`, `noon`, or `tomorrow`

-   *visualizations*: e.g., a bar chart, or Venn diagram

As we can express more complex measures in terms of simpler ones (e.g., as numbers or by some descriptive text), we can get quite far by combining our three basic data types (e.g., dates, times and locations can be described as combinations of characters and text).

#### **Shapes of data**

Beyond distinguishing data types, we can also ask about the *shape* of data or representations. While it gets challenging to answer questions like â€œHow is the numberÂ 2 represented in the brain?â€ or â€œWhat is the shape of yellow?â€, we can simplify our lives by asking: â€œIn which shape do computers represent data?â€.

1.  *scalars*: e.g., 1 or `TRUE`

2.  1-dimensional vectors and lists: e.g., `1, 2, 3` or `'x', 'y', 'z'`

3.  2-dimensional matrices: e.g., a table of health records for different people

Just as for data types, we can easily extend these basic shapes into more complex data formats, like

-   n-dimensional arrays, e.g., the number of Titanic passengers by `age`, `sex`, and `survival`

-   non-rectangular data, e.g., a list of sentences, what someone did last summer

::: callout-note
ğŸ‘‹ The contents are mostly based on **Introduction to Data Science** <https://bookdown.org/hneth/i2ds/intro.html>
:::

## An introduction to R and RStudio:

### Introduction to R and RStudio

R is a powerful and versatile programming language and environment for statistical computing and data analysis. RStudio is a popular integrated development environment (IDE) that provides a user-friendly interface for working with R.

::: callout-note
ğŸ‘‹ The R language is a free, open-source software environment for statistical computing and graphics. Download and install **R** from the Comprehensive **R** Archive Network (CRAN, [http://www.r-project.org](#0))

RStudio is an open-source integrated development environment (IDE) for R created by Posit that adds many features and productivity tools for R. . Download and install (RStudio) from <https://posit.co/products/open-source/rstudio.>
:::

### R Basics

MasteringÂ R (or any other programming language) essentially consists in solving two inter-related tasks:

1.  Defining various types of data as *objects*!

2.  Manipulating these objects by using *functions*!

#### Using R as a Calculator

You can use R as a calculator to perform basic arithmetic operations. Just type in your calculations, and R will provide the results.

```{r}

# Addition

3 + 5

# Subtraction

10 - 2

# Multiplication

4 * 7

# Division

15 / 3

```

Some additional computing numbers:

```{r}
x <- 5
y <- 2

+ x      # keeping sign 
#> [1] 5
- y      # reversing sign
#> [1] -2
x + y    # addition
#> [1] 7
x - y    # subtraction
#> [1] 3
x * y    # multiplication
#> [1] 10
x / y    # division
#> [1] 2.5
x ^ y    # exponentiation
#> [1] 25
x %/% y  # integer division
#> [1] 2
x %% y   # remainder of integer division (x mod y)
#> [1] 1
```

::: callout-note
ğŸ’¡ When an arithmetic expression contains more than one operator, the issue of *operator precedence* arises. When combining different operators, R uses the precedence rules of the so-called â€œBEDMASâ€ order:

-   **B**racketsÂ `()`,

-   **E**xponentsÂ `^`,

-   **D**ivisionÂ `/` and **M**ultiplicationÂ `*`,

-   **A**dditionÂ `+` and **S**ubtractionÂ `-`
:::

#### Creating New Objects

In simple term, in R, you can create and store values as objects and the main type of object used inÂ R for representing data is a *vector*. Vectors come in different data types and shapes and have some special properties that we need to know in order to use them in a productive fashion.

Defining an object in R is done using the assignment operator `<-`. You can name the object on the left and assign a value or expression to it on the right in the following format:

`obj_name <- value`

```{r}

# Creating objects

lg <- TRUE
n1 <- 1
n2 <- 2L
cr <- "hi"

x <- 3 * 4

y <- 3 + 4

z <- 12 / 6

a <- x * y - z

```

To determine the *type* of these objects, we can evaluate the [`typeof()`](https://rdrr.io/r/base/typeof.html) function on each of them:

```{r}
typeof(lg)
#> [1] "logical"
typeof(n1)
#> [1] "double"
typeof(n2)
#> [1] "integer"
typeof(cr)
#> [1] "character"
```

::: callout-note
âš ï¸ **Naming objects**

**Naming objects (both data objects and functions) is an art in itself.** A good general recommendation is to always aim for consistency and clarity. This may sound trivial, but if you ever tried to understand someone elseâ€™s code â€”Â including your own from a while agoÂ â€” it is astonishing how hard it actually is.

Here are some generic recommendations (some of which may be personal preferences):

-   Always aim for short but clear and descriptive names:

    -   data objects can be abstract (e.g., `abc`, `t_1`, `v_output`) or short words or abbreviations (e.g., `data`, `cur_df`),

    -   functions should be verbs (like [`print()`](https://rdrr.io/r/base/print.html)) or composita (e.g., `plot_bar()`, `write_data()`).

<!-- -->

-   Honor existing conventions (e.g., using `v`Â for vectors, `i`Â and `j`Â for indices, `x`Â andÂ `y` for coordinates, `n`Â or `N`Â for sample or population sizes, â€¦).

-   Create new conventions when this promotes consistency (e.g., giving objects that belong together similar names, or calling all functions that plot something with `plot_...()`, that compute something with `comp_...()`, etc.).

-   Use only lowercase letters and numbers for names (as they are easy to type â€”Â and absolutely avoid all special characters, as they may not exist or look very different on other peopleâ€™s computers),

-   Use `snake_case` for combined names, rather than `camelCase`, and â€”Â perhaps most importantlyÂ â€”

-   Break any of those rules if there are good (i.e., justifiable) reasons for this.
:::

#### Functions

Objects come in different data â€œtypesâ€ (e.g., character, numeric, logical) and â€œshapesâ€ (short or long), and â€”Â like any other data objectÂ â€” are manipulated by suitable â€œfunctionsâ€. R has a vast collection of built-in functions, each with specific purposes. Functions are called by their names followed by arguments in parentheses.

> *To understand computations in R, two slogans are helpful:\
> - Everything that exists is **an object**.\
> - Everything that happens is **a function** call.*
>
> John Chambers

To do something with these objects, we can apply other functions to them:

```{r}
!lg        # negate a logical value
#> [1] FALSE
n1         # print an object's current value
#> [1] 1
n1 + n2    # add 2 numeric objects
#> [1] 3
nchar(cr)  # number of characters
#> [1] 2
```

```{r}
# Using built-in functions

seq(from = 10, to = 68, by = 3) # Sequence of numbers

rep(3, 6) # Repeat a value

```

You can also create your own functions in R.

```{r }

# Defining a custom function

ifnegative <- function(x) {

if (x < 0)

print("negative")

else

print("positive")

}

# Calling the custom function

ifnegative(-5)

ifnegative(2)

```

Doing statistics inÂ R essentially means to apply statistical functions to data objects. The following basic functions examine and describe a numeric data objectÂ `nums`:

```{r}
# Define a numeric vector:
nums <- c(-10, 0, 2, 4, 6)

# basic functions:
length(nums) # nr. of elements
#> [1] 5
min(nums)    # minimum 
#> [1] -10
max(nums)    # maximum
#> [1] 6
range(nums)  # min - max
#> [1] -10   6

# aggregation functions:
sum(nums)   # sum
#> [1] 2
mean(nums)  # mean
#> [1] 0.4
var(nums)   # variance
#> [1] 38.8
sd(nums)    # standard deviation
#> [1] 6.228965
```

#### Operators

R supports various operators for arithmetic, comparison, and logical operations. Here are some commonly used operators:

#### Arithmetic Operators

-   `+` (Addition)

-   `-` (Subtraction)

-   `*` (Multiplication)

-   `/` (Division)

-   `^` or `**` (Exponentiation)

-   `%%` (Modulus)

#### Comparison Operators

-   `<` (Less than)

-   `>` (Greater than)

-   `<=` (Less than or equal to)

-   `=` (Greater than or equal to)

-   `==` (Equal)

-   `!=` (Not equal)

#### Logical Operators

-   `!` (NOT)

-   `|` (OR)

-   `&` (AND)

-   `isTRUE` (Test if an expression is TRUE)

-   isFALSE (Test if an expression is FALSE)

::: callout-note
âš ï¸ The `|` is an â€œorâ€ operator that operates on each element of a vector, while the `||` is another â€œorâ€ operator that stops evaluation the first time that the result is true!
:::

#### Colon Operator

The colon `:` operator is used to create sequences of numbers, making it helpful for creating numeric vectors.

```{r, echo=TRUE, include=TRUE}

# Creating a sequence of numbers

1:10 # Generates numbers from 1 to 10

```

#### Checker `%in%` Operator

The `%in%` operator checks if elements belong to a vector and is often used for data manipulation.

```{r}

# Checking if elements belong to a vector

a <- c(1, 2, 3, 4, 5)

b <- c(3, 4, 5, 6, 7)

a %in% b # Check if elements in 'a' belong to 'b'

```

### Data Types in R

For any data object, we distinguish between its *shape* and its *type*. The *shape* of an object mostly depends on its *structure*. Overall, the following data *types* are the one you probably encounter!

1.  numbers (of type *integer* or *double*)

2.  text or string data (of type *character*)

3.  logical values (aka. Boolean values, of type *logical*)

4.  dates and times (with various data types)

**Numeric**: These are numeric values (e.g., 3.14, 42).

```{r}

x <- c(1, 2, 3, 4.5, 6.7)

```

**Character**: These are text values (e.g., "apple," "banana").

```{r}

y <- c("apple", "banana", "cherry")
```

**Logical:** These are binary values (e.g., TRUE or FALSE).

```{r}

z <- c(TRUE, FALSE, TRUE, FALSE, TRUE)

```

**Date:** These are date values in specific format. Dates and times are more complicated data types â€”Â not because they are complicated *per se*, but because their definition and interpretation needs to account for a lot of context and conventions, plus some irregularities. At this early point in ourÂ R careers, we only need to know that such data types exist. Two particular functions allow to illustrate them and are quite useful, as they provide the current date and time:

```{r}
Sys.Date()
#> [1] "2022-09-07"
Sys.time()
#> [1] "2022-09-07 20:01:34 CEST"
```

To check the *type* of a data object, two elementary functions that can be applied to any R object are [`typeof()`](https://rdrr.io/r/base/typeof.html) and [`mode()`](https://rdrr.io/r/base/mode.html):

```{r}
typeof(TRUE)
#> [1] "logical"
typeof(10L)
#> [1] "integer"
typeof(10)
#> [1] "double"
typeof("oops")
#> [1] "character"

mode(TRUE)
#> [1] "logical"
mode(10L)
#> [1] "numeric"
mode(10)
#> [1] "numeric"
mode("oops")
#> [1] "character"
```

#### **Missing values**

The final concept considered here is not a type of data, but a type of *value*. What happens when we do not know the value of a variable? InÂ R, a lacking or missing value is represented by `NA`, which stands for *not available*, *not applicable*, or *missing value*:

```{r}
# Assign a missing value:
ms <- NA
ms
#> [1] NA

# Data type?
typeof(ms)
#> [1] "logical"
mode(ms)
#> [1] "logical"
```

As missing values are quite common in real-world data, we need to know how to deal with them. The function [`is.na()`](https://rdrr.io/r/base/NA.html) allows us to test for a missing value:

```{r}
is.na(12)
#> [1] FALSE
is.na(NA)
#> [1] TRUE
```

InÂ R, `NA` values are typically â€œaddictiveâ€ in the sense of creating more `NA` values when applying functions to them:

```{r}
NA + 1
#> [1] NA
sum(1, 2, NA, 4)
#> [1] NA
```

but many functions have ways of instructing R to ignore missing values. For instance, many numeric functions accept a logical argument `na.rm` that remove any `NA` values:

```{r}
sum(1, 2, NA, 4, na.rm = TRUE)
#> [1] 7
```

### Data Structures

|     | Homogeneous   | Heterogeneous |
|-----|---------------|---------------|
| 1d  | Atomic Vector | List          |
| 2d  | Matrix        | Data Frame    |
| nd  | Array         |               |

: R offers various data structures to store and manipulate data:

#### Atomic Vectors

\- Atomic vectors store homogeneous data, meaning all elements are of the same data type.

```{r}

# Create an atomic vector

var1 <- c(1, 2.5, 4, 6)
```

We have already seen that using the assignment operatorÂ `<-` creates new data objects and that the [`c()`](https://rdrr.io/r/base/c.html) function combines (or concatenates) objects into vectors. When the objects being combined are already stored as vectors, we are actually creating longer vectors out of shorter ones:

```{r}
# Combining scalar objects and vectors (into longer vectors): 
v1 <- 1          # same as v1 <- c(1)
v2 <- c(2, 3)

v3 <- c(v1, v2, 4)  # but the result is only 1 vector, not 2 or 3: 
v3
#> [1] 1 2 3 4
```

#### Lists

\- Lists store heterogeneous data, allowing different data types in the same list.

```{r}

# Create a list

var2 <- list("name", 2, 3, c("item1", "item2"), 16, 75)
```

#### Matrices

\- Matrices are two-dimensional data structures.

```{r}

# Create a matrix

a <- matrix(1:6, ncol = 3, nrow = 2)

```

\- Data frames are a versatile and commonly used data structure in R. They are similar to matrices but can store both numeric and non-numeric data. Data frames are often used to store datasets. Data frames are particularly useful for working with tabular data, and you can perform a wide range of operations on them.

```{r}
# create a data frame
data <- data.frame(
  Name = c("Alice", "Bob", "Charlie"),
  Age = c(25, 30, 22),
  Score = c(95, 88, 73)
)
```

## **Get your data into R**

R have multiple in-built data

```{r}
# to see which data you have
data()

# for instance, you can check out data
mtcars

```

### Reading existing local data

```{r, size="small"}
# read csv file
mydata <- read.csv("~/Desktop/mac_projects 2/data-science-with-R/data/cars.csv") 
View(mydata)
# write csv
write.csv(mydata, file = 
            "~/Desktop/mac_projects 2/data-science-with-R/data/mydata.csv")

# read csv-file from net
mydata <- read.csv(
  "https://gist.githubusercontent.com/MorganZhang100/0c489d1f376a04d5436a/raw/7c335ebe48e5751f1334bb6e4502254e3c1d1c55/cars.csv")
View(mydata)

# read excel
library(readxl)
mydata1 <- read_excel(
  "~/Desktop/mac_projects 2/data-science-with-R/data/datacom.xlsx")
View(mydata1)

```

::: callout-tip
ğŸ’¡You can use here() package in order to short-cut reading and writng data!

The "here" package in R is a useful tool for creating file paths in a project-agnostic way. It provides a simple and consistent method for specifying file paths in your R scripts and projects, making your code more portable and less prone to errors when you share it with others or move it between different machines.

Here are some key aspects and benefits of the "here" package:

**1. Platform-Independent Paths:**

-   "here" automatically generates file paths that are platform-independent. This means your code will work seamlessly on Windows, macOS, and Linux without the need to manually adjust path separators (e.g., using "\\\\" or "/" depending on the operating system).

**2. Project-Agnostic Paths:**

-   "here" is designed to work within R projects. It helps you create paths relative to the root directory of your project, making your code independent of the specific folder structure on your machine.

**3. Consistency:**

-   By using "here" to specify file paths, you ensure consistency across your project. This reduces the likelihood of errors caused by incorrect or inconsistent path specifications.

**4. Easy Integration:**

-   The "here" package can be seamlessly integrated into your R scripts and projects. You don't need to install any additional dependencies or libraries.

Here's how you can use the "here" package in R:

**1. Installation and Loading:**

-   You can install the "here" package from CRAN using the following command:

```{r}
 # install.packages("here")
  library(here)
```

**2. Creating & using Paths:**

-   To create file paths relative to your project's root directory, use the \`here()\` function. For example, to specify a path to a data file in your project, you can do the following:

```{r}
dpath <- "~/Desktop/mac_projects 2/data-science-with-R/data"
mydata2 <- read.csv(here(dpath, "/cars.csv"))
write.csv(mydata, here(dpath, "cars2.csv"))
```

The "here" package simplifies path management in R, making your code more robust and portable. It's especially helpful in larger projects where consistent path specifications are essential for reproducibility and collaboration.
:::

## Tidy Data

First you must **import** your data into R. This typically means that you take data stored in a file, database, or web application programming interface (API), and load it into a data frame in R. If you canâ€™t get your data into R, you canâ€™t do data science on it!

Once youâ€™ve imported your data, it is a good idea to **tidy** it. Tidying your data means storing it in a consistent form that matches the semantics of the dataset with the way it is stored. In brief, when your data is tidy, each column is a variable, and each row is an observation. Tidy data is important because the consistent structure lets you focus your struggle on questions about the data, not fighting to get the data into the right form for different functions.

![](images/data-science.png)

Once you have tidy data, a common first step is to **transform** it. Transformation includes narrowing in on observations of interest (like all people in one city, or all data from the last year), creating new variables that are functions of existing variables (like computing speed from distance and time), and calculating a set of summary statistics (like counts or means). Together, tidying and transforming are called **wrangling**, because getting your data in a form thatâ€™s natural to work with often feels like a fight!

Once you have tidy data with the variables you need, there are two main engines of knowledge generation: visualisation and modelling. These have complementary strengths and weaknesses so any real analysis will iterate between them many times.

**Visualisation** is a fundamentally human activity. A good visualisation will show you things that you did not expect, or raise new questions about the data. A good visualisation might also hint that youâ€™re asking the wrong question, or you need to collect different data. Visualisations can surprise you, but donâ€™t scale particularly well because they require a human to interpret them.

**Models** are complementary tools to visualisation. Once you have made your questions sufficiently precise, you can use a model to answer them. Models are a fundamentally mathematical or computational tool, so they generally scale well. Even when they donâ€™t, itâ€™s usually cheaper to buy more computers than it is to buy more brains! But every model makes assumptions, and by its very nature a model cannot question its own assumptions. That means a model cannot fundamentally surprise you.

The last step of data science is **communication**, an absolutely critical part of any data analysis project. It doesnâ€™t matter how well your models and visualisation have led you to understand the data unless you can also communicate your results to others.

Surrounding all these tools is **programming**. Programming is a cross-cutting tool that you use in every part of the project. You donâ€™t need to be an expert programmer to be a data scientist, but learning more about programming pays off because becoming a better programmer allows you to automate common tasks, and solve new problems with greater ease.

For more see: [Modern Data Science with R - Appendix B â€” Introduction to R and RStudio (mdsr-book.github.io)](https://mdsr-book.github.io/mdsr3e/B-appR.html#appR-exercises)
