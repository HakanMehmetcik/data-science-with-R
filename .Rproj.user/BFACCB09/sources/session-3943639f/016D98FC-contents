---
title: "Week1: Basics"
author: "Hakan Mehmetcik"
format: pdf
editor: visual
execute: 
  echo: true
  warning: true
  output: asis
df-print: kable
---

```{r setup, echo=FALSE, message=FALSE}

# loaded packages
library(here)
library(tidyverse)
library(ggplot2)
library(mdsr)

here <- here::here()
dpath <- "data"
```

## **Collective Statistical Illiteracy: Understanding the Challenge**

In contemporary society, almost every news article or broadcast includes some form of scientific or numerical data. Headlines routinely mention statistics about public health, economic indicators, environmental changes, and more. However, a significant portion of the audience struggles to interpret these numbers and to understand how they were derived. This difficulty can have profound implications for personal decision-making, organizational strategy, and public policy.

The concept of **collective statistical illiteracy** highlights this widespread inability to grasp the meaning behind statistical facts and figures. When individuals lack the skills to critically evaluate quantitative information, they are more susceptible to misunderstanding important issues, making poorly informed decisions, or being swayed by misleading statistics.

> ***Statistical** thinking will one day be as necessary for efficient citizenship as the ability to read and write.*
>
> H.G. Wells (1903, paraphrased by S.S. Wilks, see [link](http://quotablemath.blogspot.com/2018/02/misquoted-hg-wells-on-statistics.html))

## **Why Study Data Science?**

Data science drives innovation and decision-making in virtually every industry‚Äîfrom healthcare and finance to social media and marketing. By combining computational methods with statistical analysis, data science uncovers hidden patterns and insights that help organizations craft evidence-based strategies, predict future trends, and make more informed decisions. As the volume of data grows exponentially, the ability to collect, process, and interpret this information becomes ever more critical.

### **How Is Data Science Different from Math and Statistics?**

-   **Interdisciplinary Approach**: While mathematics and statistics focus on theoretical models and the science of uncertainty, data science integrates computer programming, domain knowledge, and data engineering alongside statistical techniques.

-   **Practical, Data-Driven Focus**: Data scientists often work with real-world data that may be messy or unstructured. They prioritize building reproducible workflows and scalable analyses, which is not the central concern in pure math or traditional statistics.

-   **Technology and Tools**: Data science heavily relies on programming languages (e.g., Python, R) and tools (e.g., machine learning frameworks, data visualization platforms) to handle large, complex datasets‚Äîcapabilities not typically emphasized in standard math or statistics coursework.

*üöÄ **Data science***---the science of extracting meaningful information from data. As such, data science is a broader field than statistics, encompassing data gathering, preparation, modeling, visualization, computing, and meta-analysis of data science itself.

::: callout-note
An assumption underlying these efforts is that data is the foundation of various information types that can eventually be turned into knowledge and wisdom. Figure below shows their arrangement in a hierarchical structure of a [DIKW pyramid](https://en.wikipedia.org/wiki/DIKW_pyramid):

![](images/DIKW_Pyramid.svg.png)

The key distinction between the lower and the upper layers of the pyramid is that data needs to address some hypothesis or answer some question, and has to be interpreted and understood to become valuable. Here to say, another reason for the close interaction between data and theory is that we need theoretical models for understanding and interpreting data. When analyzing data, we are typically interested in the underlying mechanisms (i.e., the causal relationships between variables). Importantly, any pattern of data can be useless and misleading, when the data-generating process is unknown or ignored. Knowledge or at least assumptions regarding the causal process illuminate the data and are required for its sound interpretation. The importance of theoretical assumptions for data analysis cannot be underestimated (see, e.g., the notion of causal models and counterfactual reasoning in [Pearl & Mackenzie, 2018](https://bookdown.org/hneth/i2ds/references.html#ref-PearlMackenzie18)). Thus, **pitting data against theory is nonsense**: Using and understanding data is always based on theory.
:::

Our aims is to prepare **well-documented and reproducible** analysis since data literacy is the ability and skill of making sense of data. This includes numeracy, risk-literacy, and the ability of using tools to collect, transform, analyze, interpret, and present data, in a transparent, reproducible, and responsible fashion.

ü§î **Information** is what we want, but data are what we‚Äôve got. The techniques for transforming data into information is the ***data science!***

**Data scientists, therefore, are individuals who strive to transform the plentiful data available today into actionable information, which often appears to be in short supply**

üöÄ ***Think with data = desire to solve problems using data***

üöÄ **A data scientist** is "a knowledge worker" who is principally occupied with analyzing complex and massive data resources.

üöÄ**Demand** for data skills is strong! Data Scientist is one of the best job in the world since 2016.

üöÄThe Key components that are part of [***data acumen***]{.underline} include mathematical, computational, and statistical foundations, data management and curation, data description and visualization, data modeling and assessment, workflow and reproducibility, communication and teamwork, domain-specific considerations, and ethical problem solving.¬†

üöÄTherefore, contemporary data science requires **tight integration of statistical, computational, and communication skills.**

üöÄ**Data Wrangling** a process of preparing data for visualization and other modern techniuqes of statistical interpretations.

::: callout-note
**Recommended Background Readings**

1.  **Baumer, B. S., Kaplan, D. T., & Horton, N. J. (2021). *Modern Data Science with R*.**

    -   **Chapter 1: Prologue: Why Data Science?**

    -   This chapter introduces the motivation behind data science as a discipline. It discusses how the field has evolved, its interdisciplinary nature, and why it is relevant today. The reading helps students understand the **philosophical and practical reasons for studying data science**.

2.  **Donoho, D. (2017). "50 Years of Data Science." *Journal of Computational and Graphical Statistics*, 26(4), 745-766.**

    -   A historical perspective on the evolution of **data science as a field**, emphasizing how it extends beyond traditional statistics.

    -   Discusses the distinction between **data science and statistics**, highlighting the importance of computational tools.

    -   Advocates for a more **inclusive and interdisciplinary** approach to data science education.

3.  **De Veaux, R. D., Agarwal, M., Averett, M., Baumer, B. S., Bray, A., Bressoud, T. C., & others. (2017). "Curriculum Guidelines for Undergraduate Programs in Data Science." *Annual Review of Statistics and Its Application*, 4, 15-30.**

    -   Provides an overview of the **core competencies needed for data science education**, including programming, statistical inference, and domain expertise.

    -   Defines **guidelines for structuring an undergraduate data science curriculum**, which is useful for understanding how your course fits within broader educational frameworks.

    -   Emphasizes the importance of **ethics, reproducibility, and communication skills** in data science training.
:::

## What is data?

Today, the manner in which we extract meaning from data is different in two ways---both due primarily to advances in computing:

-   we are able to compute many more things than we could before, and,

-   we have a *lot* more data than we had before.

üöÄ **the traditional two-dimensional representation of data:** rows and columns in a data table, and horizontal and vertical in a data graphic. For instance, if someone aimed to collect or compare health-related characteristics of some people, he or she would measure and record these characteristics in some file. In such a file of health records, some values may identify persons (e.g., by name or some ID¬†code), while others describe them by numbers (e.g., their age, height, etc.) or various codes or text labels (e.g., their address, profession, diagnosis, notes on allergies, medication, vaccinations, etc.). The variables and values are typically stored in tabular form: If each of the table‚Äôs rows describes an individual person as our unit of observation, its columns are called *variables* and the entries in the cells (which can be referenced as combinations of rows and columns) are *values*. The contents of the entire table (i.e., the observations, variables, and values) are typically called ‚Äúdata‚Äù.

**Non-traditional data types (e.g., geospatial, text, network, "big") and interactive data graphic** are the new normal! Thus, we can argue that the gap between generating scientific insights and understanding them is widening.

The increasing complexity and heterogeneity of modern data means that each data analysis project needs to be custom-built. Simply put, the modern data analyst needs to be able to read and write computer instructions, the ‚Äúcode‚Äù from which data analysis projects are built.

üöÄ **Domain knowledge**¬†is always useful in data science since data science is best applied in the context of expert knowledge about the domain from which the data originate. For data scientists of all application domains, creativity, domain knowledge, and technical ability are absolutely essential.

Some related discussions in this contexts include:

1.  The distinction between *raw data* (e.g., measurements, inputs) and *processed data* (results or interpretations): Data are often portrayed as a potential resource: Something that can be collected or generated, harvested, and ‚Äî¬†by processing¬†‚Äî refined or distilled to turn it into something more valuable. The related term *data mining* also suggests that data is some passive raw material, whereas the active processing and interpretation of data (by algorithms, rules, instructions) can generate interest and revenue.

2.  The distinction between *data* and *information*: In contrast to data, the term *information* has a clear formal definition (see [Wikipedia: Information theory](https://en.wikipedia.org/wiki/Information_theory)). And while data appears to be neutral, information is usually viewed as something positive and valuable. Hence, when data is being used and perceived as useful, we can generate *insight*, *knowledge*, or perhaps even *wisdom* from it?

3.  The field of signal detection theory (SDT, see [Wikipedia: Detection theory](https://en.wikipedia.org/wiki/Detection_theory)) distinguishes between *signal* and *noise*. Does *data* include both signal and noise, or should we only count the signal parts as data?

## What is Data Science (DS)?

DS is not a single and homogeneous discipline. Instead, it overlaps and is intricately interwoven with several other academic fields and requires corresponding skills. For instance, becoming an expert in DS requires considerable knowledge in statistics, but the discipline of data science is not as mathematical as statistics. Similarly, data science involves computers, but is usually not as much concerned with abstract formalisms as computer science. We could describe DS as applied statistics or applied computer science, but this would suggest that it is a sub-discipline of these other fields ‚Äî¬†and any serious application ususally assumes knowledge of the discipline‚Äôs foundations.

![](images/ds.png)

As¬†R is defined as a ‚Äúsoftware environment for statistical computing and graphics‚Äù ([R Core Team, 2023](https://bookdown.org/hneth/i2ds/references.html#ref-R)), it supports and provides tools for all topics mentioned and is thus pictured at the center. However, this central position does not imply that R is the only or a necessary tool for DS (e.g., a popular alternative is the programming language [Python](https://www.python.org/)).

::: callout-note
üí° A striking example of bad software choices is the Public Health England (PHE)‚Äôs recent decision to import CSV-files on COVID-19 test results into Microsoft Excel‚Äôs XLS file format. Due to this file format‚Äôs artificial limit to a maximum of 65.536¬†rows of data, nearly 16.000¬†Covid cases went unreported in the U.K. (which amounts to almost 24% of the cases recorded in the time span from Sep¬†25 to Oct.¬†2, 2020).
:::

The skills for successfully dealing with data are not confined to one discipline or talent. As data scientists must discover, mine, select, organize, transform, analyze, understand, communicate and present information, they tend to be generalists, rather than specialists. Beyond a set of skills from a diverse range of areas, getting DS done requires the familiarity with and mastery of suitable tools.

## The *same* data can be represented in many *different types* and *shapes*.

#### **Types of data**

As variables and values depend on what we want to measure (i.e., our goals), it is impossible to provide a comprehensive list of data types. Nevertheless, computer scientists typically categorize data into different types. This is made possible by distinguishing between different ways in which data is represented. The three most basic types of data are:

1.  *Truth values* (aka. *logicals*): either `TRUE` or `FALSE`

2.  *Numbers*: e.g., 2,12,‚àö22,12,2

3.  *Text* (aka. *characters* or *strings*): e.g., ‚ÄúDonald Duck‚Äù or ‚ÄúWar and Peace‚Äù

Most computer languages have ways to represent these three elementary types. Exactly how truth values, numbers, or text relate to the actual phenomena being described involves many issues of representation and measurement.

In addition to these three basic data types, there are common data types that have precise and widely-shared definitions, like

-   *dates* and *times*: e.g., 2025-02-10, `11:55`

-   *locations*: e.g., the summit of Mount Everest, or N40‚àò‚àò 44.9064‚Äô, W073‚àò‚àò 59.0735‚Äô

but even more potential data types that we could define, if we had or wanted to, for instance

-   *temporal terms*: e.g., `Monday`, `noon`, or `tomorrow`

-   *visualizations*: e.g., a bar chart, or Venn diagram

As we can express more complex measures in terms of simpler ones (e.g., as numbers or by some descriptive text), we can get quite far by combining our three basic data types (e.g., dates, times and locations can be described as combinations of characters and text).

#### **Shapes of data**

Beyond distinguishing data types, we can also ask about the *shape* of data or representations. While it gets challenging to answer questions like ‚ÄúHow is the number¬†2 represented in the brain?‚Äù or ‚ÄúWhat is the shape of yellow?‚Äù, we can simplify our lives by asking: ‚ÄúIn which shape do computers represent data?‚Äù.

1.  *scalars*: e.g., 1 or `TRUE`

2.  1-dimensional vectors and lists: e.g., `1, 2, 3` or `'x', 'y', 'z'`

3.  2-dimensional matrices: e.g., a table of health records for different people

Just as for data types, we can easily extend these basic shapes into more complex data formats, like

-   n-dimensional arrays, e.g., the number of Titanic passengers by `age`, `sex`, and `survival`

-   non-rectangular data, e.g., a list of sentences, what someone did last summer

For more see: **Introduction to Data Science** <https://bookdown.org/hneth/i2ds/intro.html>

## An introduction to R and RStudio:

### Introduction to R and RStudio

R is a powerful and versatile programming language and environment for statistical computing and data analysis. RStudio is a popular integrated development environment (IDE) that provides a user-friendly interface for working with R.

::: callout-note
üëã The R language is a free, open-source software environment for statistical computing and graphics. Download and install **R** from the Comprehensive **R** Archive Network (CRAN, [http://www.r-project.org](#0))

RStudio is an open-source integrated development environment (IDE) for R created by Posit that adds many features and productivity tools for R. . Download and install (RStudio) from <https://posit.co/products/open-source/rstudio.>
:::

### R Basics

#### Using R as a Calculator

You can use R as a calculator to perform basic arithmetic operations. Just type in your calculations, and R will provide the results.

```{r}

# Addition

3 + 5

# Subtraction

10 - 2

# Multiplication

4 * 7

# Division

15 / 3

```

#### Creating New Objects

In R, you can create and store values in objects. This is done using the assignment operator `<-`. You can name the object on the left and assign a value or expression to it on the right.

```{r}

# Creating objects

x <- 3 * 4

y <- 3 + 4

z <- 12 / 6

a <- x * y - z

```

#### Functions

R has a vast collection of built-in functions, each with specific purposes. Functions are called by their names followed by arguments in parentheses.

```{r}

# Using built-in functions

seq(from = 10, to = 68, by = 3) # Sequence of numbers

rep(3, 6) # Repeat a value

```

You can also create your own functions in R.

```{r }

# Defining a custom function

ifnegative <- function(x) {

if (x < 0)

print("negative")

else

print("positive")

}

# Calling the custom function

ifnegative(-5)

ifnegative(2)

```

#### Operators

R supports various operators for arithmetic, comparison, and logical operations. Here are some commonly used operators:

#### Arithmetic Operators

-   `+` (Addition)

-   `-` (Subtraction)

-   `*` (Multiplication)

-   `/` (Division)

-   `^` or `**` (Exponentiation)

-   `%%` (Modulus)

#### Comparison Operators

-   `<` (Less than)

-   `>` (Greater than)

-   `<=` (Less than or equal to)

-   `=` (Greater than or equal to)

-   `==` (Equal)

-   `!=` (Not equal)

#### Logical Operators

-   `!` (NOT)

-   `|` (OR)

-   `&` (AND)

-   `isTRUE` (Test if an expression is TRUE)

-   isFALSE (Test if an expression is FALSE)

::: callout-note
‚ö†Ô∏è The `|` is an ‚Äúor‚Äù operator that operates on each element of a vector, while the `||` is another ‚Äúor‚Äù operator that stops evaluation the first time that the result is true!
:::

#### Colon Operator

The colon `:` operator is used to create sequences of numbers, making it helpful for creating numeric vectors.

```{r, echo=TRUE, include=TRUE}

# Creating a sequence of numbers

1:10 # Generates numbers from 1 to 10

```

#### Checker `%in%` Operator

The `%in%` operator checks if elements belong to a vector and is often used for data manipulation.

```{r}

# Checking if elements belong to a vector

a <- c(1, 2, 3, 4, 5)

b <- c(3, 4, 5, 6, 7)

a %in% b # Check if elements in 'a' belong to 'b'

```

### Data Types in R

R supports different data types, including:

\- **Numeric**: These are numeric values (e.g., 3.14, 42).

```{r}

x <- c(1, 2, 3, 4.5, 6.7)

```

**Character**: These are text values (e.g., "apple," "banana").

```{r}

y <- c("apple", "banana", "cherry")
```

**Logical:** These are binary values (e.g., TRUE or FALSE).

```{r}

z <- c(TRUE, FALSE, TRUE, FALSE, TRUE)

```

### Data Structures

|     | Homogeneous   | Heterogeneous |
|-----|---------------|---------------|
| 1d  | Atomic Vector | List          |
| 2d  | Matrix        | Data Frame    |
| nd  | Array         |               |

: R offers various data structures to store and manipulate data:

#### Atomic Vectors

\- Atomic vectors store homogeneous data, meaning all elements are of the same data type.

```{r}

# Create an atomic vector

var1 <- c(1, 2.5, 4, 6)
```

#### Lists

\- Lists store heterogeneous data, allowing different data types in the same list.

```{r}

# Create a list

var2 <- list("name", 2, 3, c("item1", "item2"), 16, 75)
```

#### Matrices

\- Matrices are two-dimensional data structures.

```{r}

# Create a matrix

a <- matrix(1:6, ncol = 3, nrow = 2)

```

\- Data frames are a versatile and commonly used data structure in R. They are similar to matrices but can store both numeric and non-numeric data. Data frames are often used to store datasets. Data frames are particularly useful for working with tabular data, and you can perform a wide range of operations on them.

```{r}
# create a data frame
data <- data.frame(
  Name = c("Alice", "Bob", "Charlie"),
  Age = c(25, 30, 22),
  Score = c(95, 88, 73)
)
```

## **Get your data into R**

R have multiple in-built data

```{r}
# to see which data you have
data()

# for instance, you can check out data
mtcars

```

### Reading existing local data

```{r, size="small"}
# read csv file
mydata <- read.csv("~/Desktop/mac_projects 2/data-science-with-R/data/cars.csv") 
View(mydata)
# write csv
write.csv(mydata, file = 
            "~/Desktop/mac_projects 2/data-science-with-R/data/mydata.csv")

# read csv-file from net
mydata <- read.csv(
  "https://gist.githubusercontent.com/MorganZhang100/0c489d1f376a04d5436a/raw/7c335ebe48e5751f1334bb6e4502254e3c1d1c55/cars.csv")
View(mydata)

# read excel
library(readxl)
mydata1 <- read_excel(
  "~/Desktop/mac_projects 2/data-science-with-R/data/datacom.xlsx")
View(mydata1)

```

::: callout-tip
üí°You can use here() package in order to short-cut reading and writng data!

The "here" package in R is a useful tool for creating file paths in a project-agnostic way. It provides a simple and consistent method for specifying file paths in your R scripts and projects, making your code more portable and less prone to errors when you share it with others or move it between different machines.

Here are some key aspects and benefits of the "here" package:

**1. Platform-Independent Paths:**

-   "here" automatically generates file paths that are platform-independent. This means your code will work seamlessly on Windows, macOS, and Linux without the need to manually adjust path separators (e.g., using "\\\\" or "/" depending on the operating system).

**2. Project-Agnostic Paths:**

-   "here" is designed to work within R projects. It helps you create paths relative to the root directory of your project, making your code independent of the specific folder structure on your machine.

**3. Consistency:**

-   By using "here" to specify file paths, you ensure consistency across your project. This reduces the likelihood of errors caused by incorrect or inconsistent path specifications.

**4. Easy Integration:**

-   The "here" package can be seamlessly integrated into your R scripts and projects. You don't need to install any additional dependencies or libraries.

Here's how you can use the "here" package in R:

**1. Installation and Loading:**

-   You can install the "here" package from CRAN using the following command:

```{r}
 # install.packages("here")
  library(here)
```

**2. Creating & using Paths:**

-   To create file paths relative to your project's root directory, use the \`here()\` function. For example, to specify a path to a data file in your project, you can do the following:

```{r}
dpath <- "~/Desktop/mac_projects 2/data-science-with-R/data"
mydata2 <- read.csv(here(dpath, "/cars.csv"))
write.csv(mydata, here(dpath, "cars2.csv"))
```

The "here" package simplifies path management in R, making your code more robust and portable. It's especially helpful in larger projects where consistent path specifications are essential for reproducibility and collaboration.
:::

## Tidy Data

First you must **import** your data into R. This typically means that you take data stored in a file, database, or web application programming interface (API), and load it into a data frame in R. If you can‚Äôt get your data into R, you can‚Äôt do data science on it!

Once you‚Äôve imported your data, it is a good idea to **tidy** it. Tidying your data means storing it in a consistent form that matches the semantics of the dataset with the way it is stored. In brief, when your data is tidy, each column is a variable, and each row is an observation. Tidy data is important because the consistent structure lets you focus your struggle on questions about the data, not fighting to get the data into the right form for different functions.

![](images/data-science.png)

Once you have tidy data, a common first step is to **transform** it. Transformation includes narrowing in on observations of interest (like all people in one city, or all data from the last year), creating new variables that are functions of existing variables (like computing speed from distance and time), and calculating a set of summary statistics (like counts or means). Together, tidying and transforming are called **wrangling**, because getting your data in a form that‚Äôs natural to work with often feels like a fight!

Once you have tidy data with the variables you need, there are two main engines of knowledge generation: visualisation and modelling. These have complementary strengths and weaknesses so any real analysis will iterate between them many times.

**Visualisation** is a fundamentally human activity. A good visualisation will show you things that you did not expect, or raise new questions about the data. A good visualisation might also hint that you‚Äôre asking the wrong question, or you need to collect different data. Visualisations can surprise you, but don‚Äôt scale particularly well because they require a human to interpret them.

**Models** are complementary tools to visualisation. Once you have made your questions sufficiently precise, you can use a model to answer them. Models are a fundamentally mathematical or computational tool, so they generally scale well. Even when they don‚Äôt, it‚Äôs usually cheaper to buy more computers than it is to buy more brains! But every model makes assumptions, and by its very nature a model cannot question its own assumptions. That means a model cannot fundamentally surprise you.

The last step of data science is **communication**, an absolutely critical part of any data analysis project. It doesn‚Äôt matter how well your models and visualisation have led you to understand the data unless you can also communicate your results to others.

Surrounding all these tools is **programming**. Programming is a cross-cutting tool that you use in every part of the project. You don‚Äôt need to be an expert programmer to be a data scientist, but learning more about programming pays off because becoming a better programmer allows you to automate common tasks, and solve new problems with greater ease.

For more see: [Modern Data Science with R - Appendix B ‚Äî Introduction to R and RStudio (mdsr-book.github.io)](https://mdsr-book.github.io/mdsr3e/B-appR.html#appR-exercises)
