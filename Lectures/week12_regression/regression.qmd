---
title: "Inferenatial Statistics: Regression"
author: "Hakan Mehmetcik"
format: pdf
editor: visual
execute: 
  echo: true
  warning: true
  output: asis
df-print: kable
---

# Functions and Models

## Functions

A function: You take some kind of input and you get some kind of output!

The hint here is that you can get ONE specific output from any specific input.

Ex: **A national park contains foxes that prey on rabbits.  The table below gives the two populations, F and R, over an 11-month period, where t=0 means January, t=1 means February, and so on.**

Sure, here is the table in R markdown format:

| Month | Rabbits | Foxes |
|-------|---------|-------|
| 0     | 1,000   | 150   |
| 1     | 750     | 143   |
| 2     | 567     | 125   |
| 3     | 500     | 100   |
| 4     | 567     | 75    |
| 5     | 750     | 57    |
| 6     | 1,000   | 50    |
| 7     | 1,250   | 57    |
| 8     | 1,433   | 75    |
| 9     | 1,500   | 100   |
| 10    | 1,433   | 125   |

This table shows the number of rabbits and foxes over a period of 10 months. The 'Rabbits' and 'Foxes' columns represent the population of each species at the end of each month.

1.  Is F a function of t?

Yes, because for each value of t, there is axactly one value of F.

2.  Is R a function of F? 

No, beacuse when F=57 R=750 or R=1250.

💡 A function is generally denoted by f(x) where x is the input. For example, in the function f(x) = x\^2, the function f(x) takes the value of "x" and then squares it!

👋 In statistics, **a distribution is a function that shows the possible values for a variable and how often they occur**. It's a description of the relative numbers of times each possible outcome will occur in a number of trials. Distributions can be represented in various ways, such as with graphs or probability tables.

## Statisitcal Modeling

Statistical modeling provides a way to relate variables to one another. Doing so helps us better understand the system we are studying. Regression is the simplest modeling in statistic.

## Introduction to Regression Analysis

Regression analysis is a fundamental method used in inferential statistics, complementing estimation and hypothesis testing. It is primarily employed to examine relationships between variables and to develop models that can predict one variable based on others.

## Types of Regression

### 1. Linear Regression

This is the simplest form where the relationship between the dependent variable and one or more independent variables is assumed to be linear. It is expressed as:

$$ Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_nX_n + \epsilon $$

Here, `Y` is the dependent variable, `X_1, X_2, ..., X_n` are the independent variables, `\beta_0, \beta_1, ..., \beta_n` are the coefficients, and `\epsilon` is the error term.

### 2. Multiple Regression

Extends linear regression by using multiple independent variables to better predict the outcome.

### 3. Logistic Regression

Used when the dependent variable is categorical, typically binary, to predict the probability of an event's occurrence by fitting data to a logistic curve.

## Regression as Inferential Statistics

Regression analysis in inferential statistics is utilized to:

-   **Estimate Relationships**: It helps in determining the strength and significance of relationships between variables.
-   **Predict Outcomes**: Based on the regression model, it predicts values of the dependent variable for given values of independent variables.
-   **Test Hypotheses**: It tests specific hypotheses about the relationships among variables, such as whether the impact of predictors is significant.

## Example: Regression for Inference

Consider a scenario where a researcher is investigating the effect of study hours (`X`) on test scores (`Y`). The simple linear regression model can be:

$$ Y = \beta_0 + \beta_1X + \epsilon $$

Where:

-   `\beta_0` is the intercept (expected test score when study hours are zero),
-   `\beta_1` is the slope (expected increase in test score for each additional hour of study),
-   `\epsilon` is the error term.

### R Code for Fitting the Model

Statistical models to explore the relationship a response (dependent) and explanatory (independent variable)With regression, you can predict the values of the response variable with the use of given values of explanatory variable.

👋 Make sure that you know the followings!

-   **linear regression :** when the response variable is numeric

-   **logistic regression:** when the response variable is logical (True -false)

-   **simple linear/logistic regression:** when there is one explanatory variable

-   **multiple linear/logistic regression:** when there is more than one explanatory variable

-   👋 **lm() function to simulate linear models in R:**In R, linear models are typically fitted using the `lm()` function. This function fits a linear model to data by finding the line of best fit that minimizes the total error of the model.

    ```{r}
    # Sample Data
    set.seed(123)
    study_hours <- runif(100, 0, 10)  # Randomly generate study hours between 0 and 10
    test_scores <- 50 + 5 * study_hours + rnorm(100, mean=0, sd=5)  # Generate test scores

    # Fit Linear Regression Model
    model <- lm(test_scores ~ study_hours)
    summary(model)
    ```

In this code, `lm(y ~ x)` fits a linear model to the data with `y` as the response variable and `x` as the predictor variable. The `summary(model)` function then prints a summary of the model, including the coefficients, residuals, and other diagnostic measures.

⚠️ The **`summary()`** function provides a lot of information. Here's what some of it means:

-   **Call**: This shows the function call that you used to fit the model. (`lm(y ~ x) in the example above)`

-   **Residuals**: This section provides summary statistics for the residuals, which are a measure of how far off our model's predictions are for each point.

-   **Coefficients**: This is one of the most important parts of the output. It gives the coefficients of the model, the standard error of these coefficients, and the t-value and p-value of the hypothesis test that the coefficient is different from 0. The "Estimate" column gives the coefficients of the intercept and the predictor variable(s). In this case, the coefficient for **`x`** is the slope of the line of best fit.

-   **Residual standard error**: This is the standard deviation of the residuals. It gives a measure of how wrong our model's predictions are likely to be.

-   **Multiple R-squared**: This is the proportion of variance in the response variable that can be explained by the predictor variables. It provides a measure of how well the model fits the data.

-   **F-statistic and p-value**: The F-statistic is a measure of how much better the model fits the data than a model that has no predictor variables. The p-value associated with this F-statistic is the probability of observing such an F-statistic, or one more extreme, under the null hypothesis that the model with no predictor variables fits the data as well as our model.

## Exercise 1:

For this exercise, we have bivariate data on a group of college students: the total amount (in dollars) spent on textbooks throughout their college career, and their GPA. The following linear regression model was used to predict GPA from number of dollars (in hundreds) spent:

**Predicted GPA = 2.84 + .04\*Dollars**

### Question 1:

What is the predicted GPA of a student who spent a total of \$970 on textbooks in college?

```{r}
predictedgpa <- function(x) {
  2.84 + 0.04*x/100
}

predictedgpa(970)
```

### Question 2:

If a student spent \$0 on textbooks in college and graduated with a GPA of 3.71, what is her residual? 

```{r}
# residual = true-value - predicted_value 

3.71- predictedgpa(0)
```

### Question 3:

If a student spent \$1,450 on textbooks and graduated with a GPA of 2.91, what is his residual? *(Please indicate whether the residual is positive or negative in your response, and **round to 2 decimal places**.)*

```{r}
2.91 - predictedgpa(1450)
```

### Question 4:

Calculate how much money a freshman has to spend to get 4 GPA

```{r}
# 4 = 2.84 + 0.04*x/100
((4-2.84)/0.04)*100
```

A freshman learned of this study and calculated that she would need to spend \$2,900 on textbooks to earn a 4.0 GPA. (You have just confirmed this calculation using the equation above). She decided to buy all of her textbooks new (rather than second-hand and cheaper) to help boost her GPA. Is she using the model in a statistically-sound way?

**Answer** : Of course not. No!

## Exercise 2:

let’s consider another question from the airline delays data set: What impact, if any, does scheduled time of departure have on expected flight delay? Many people think that earlier flights are less likely to be delayed, since flight delays tend to cascade over the course of the day. Is this theory supported by the data?

We first begin by considering time of day. In the **nycflights13** package, the `flights` data frame has a variable (`hour`) that specifies the *scheduled* hour of departure.

```{r}
library(tidyverse)
library(mdsr)
library(nycflights13)
SF <- flights |>
  filter(dest == "SFO", !is.na(arr_delay))
SF |>
  group_by(hour) |>
  count() |>
  pivot_wider(names_from = hour, values_from = n) |>
  data.frame()
```

We see that many flights are scheduled in the early to mid-morning and from the late afternoon to early evening. None are scheduled before 5 am or after 10 pm. Let’s examine how the arrival delay depends on the hour. We’ll do this in two ways: first using standard box-and-whisker plots to show the distribution of arrival delays; second with a [*linear model*](https://en.wikipedia.org/w/index.php?search=linear%20model) that lets us track the mean arrival delay over the course of the day.

```{r}
SF |>
  ggplot(aes(x = hour, y = arr_delay)) +
  geom_boxplot(alpha = 0.1, aes(group = hour)) +
  geom_smooth(method = "lm") + 
  xlab("Scheduled hour of departure") + 
  ylab("Arrival delay (minutes)") + 
  coord_cartesian(ylim = c(-30, 120)) 
```

Figure above displays the arrival delay versus schedule departure hour. The average arrival delay increases over the course of the day. The trend line itself is created via a regression model.

```{r}
mod1 <- lm(arr_delay ~ hour, data = SF)
broom::tidy(mod1)
```

The number under the “estimate” for `hour` indicates that the arrival delay is predicted to be about 2 minutes higher per hour. Over the 15 hours of flights, this leads to a 30-minute increase in arrival delay comparing flights at the end of the day to flights at the beginning of the day. The `tidy()` function from the **broom** package also calculates the standard error: 0.09 minutes per hour. Stated as a 95% confidence interval, this model indicates that we are 95% confident that the true arrival delay increases by 2.0±0.18 minutes per hour.

The rightmost column gives the [*p-value*](https://en.wikipedia.org/w/index.php?search=p-value), a way of translating the estimate and standard error onto a scale from zero to one. By convention, p-values below 0.05 provide a kind of certificate testifying that random, accidental patterns would be unlikely to generate an estimate as large as that observed. The tiny p-value given in the report (`2e-16` is 0.0000000000000002) is another way of saying that if there was no association between time of day and flight delays, we would be *very* unlikely to see a result this extreme or more extreme.

Can we do better? What additional factors might help to explain flight delays? Let’s look at departure airport, carrier (airline), month of the year, and day of the week. Some wrangling will let us extract the day of the week (dow) from the year, month, and day of month. We’ll also create a variable season that summarizes what we already know about the month: that June and July are the months with long delays. These will be used as explanatory variables to account for the response variable: arrival delay.

```{r}
library(lubridate)
SF <- SF |> 
  mutate(
    day = as.Date(time_hour), 
    dow = as.character(wday(day, label = TRUE)),
    season = ifelse(month %in% 6:7, "summer", "other month")
  )
```

Now we can build a model that includes variables we want to use to explain arrival delay.

```{r}
mod2 <- lm(arr_delay ~ hour + origin + carrier + season + dow, data = SF)
broom::tidy(mod2)
```

The numbers in the “estimate” column tell us that we should add 4.1 minutes to the average delay if departing from `JFK` (instead of `EWR`, also known as [*Newark*](https://en.wikipedia.org/w/index.php?search=Newark), which is the reference group). Delta has a better average delay than the other carriers. Delays are on average longer in June and July (by 25 minutes), and on Sundays (by 5 minutes). Recall that the Aviana crash was in July.

The model also indicates that Sundays are associated with roughly 5 minutes of additional delays; Saturdays are 6 minutes less delayed on average. (Each of the days of the week is being compared to Friday, chosen as the reference group because it comes first alphabetically.) The standard errors tell us the precision of these estimates; the p-values describe whether the individual patterns are consistent with what might be expected to occur by accident even if there were no systemic association between the variables.

In this example, we’ve used `lm()` to construct what are called [*linear models*](https://en.wikipedia.org/w/index.php?search=linear%20models). Linear models describe how the mean of the response variable varies with the explanatory variables. They are the most widely used statistical modeling technique, but there are others.

In particular, since our original motivation was to set a policy about business travel, we might want a modeling technique that lets us look at another question: What is the probability that a flight will be, say, greater than 100 minutes late? Without going into detail, we’ll mention that a technique called [*logistic regression*](https://en.wikipedia.org/w/index.php?search=logistic%20regression) is appropriate for such [*dichotomous*](https://en.wikipedia.org/w/index.php?search=dichotomous) outcomes.

### **Confounding and accounting for other factors**

We drill the mantra [*correlation does not imply causation*](https://en.wikipedia.org/w/index.php?search=correlation%20does%20not%20imply%20causation) into students whenever statistics are discussed. While the statement is certainly true, it may not be so helpful.

There are many times when correlations *do* imply causal relationships (beyond just in carefully conducted [*randomized trials*](https://en.wikipedia.org/w/index.php?search=randomized%20trials)). A major concern for observational data is whether the true associations are being distorted by *other factors* that may be the actual determinants of the observed relationship between two factors. Such other factors may [*confound*](https://en.wikipedia.org/w/index.php?search=confound) the relationship being studied.

Randomized trials in scientific experiments are considered the gold standard for evidence-based research. Such trials, sometimes called [*A/B tests*](https://en.wikipedia.org/w/index.php?search=A/B%20tests), are commonly undertaken to compare the effect of a treatment (e.g., two different forms of a Web page). By controlling who receives a new intervention and who receives a control (or standard treatment), the investigator ensures that, on average, all other factors are balanced between the two groups. This allows them to conclude that if there are differences in the outcomes measured at the end of the trial, they can be attributed to the application of the treatment. (It’s worth noting that randomized trials can also have confounding if subjects don’t comply with treatments or are lost on follow-up.)

While they are ideal, randomized trials are not practical in many settings. It is not ethical to randomize some children to smoke and the others not to smoke in order to determine whether cigarettes cause lung cancer. It is not practical to randomize adults to either drink coffee or abstain to determine whether it has long-term health impacts. Observational (or “found”) data may be the only feasible way to answer important questions.

Let’s consider an example of confounding using observational data on average teacher salaries (in 2010) and average total SAT scores for each of the 50 United States. The SAT ([*Scholastic Aptitude Test*](https://en.wikipedia.org/w/index.php?search=Scholastic%20Aptitude%20Test)) is a high-stakes exam used for entry into college. Are higher teacher salaries associated with better outcomes on the test at the state level? If so, should we adjust salaries to improve test performance? [Figure 9.4](https://mdsr-book.github.io/mdsr3e/09-foundations.html#fig-sat1) displays a scatterplot of these data. We also fit a linear regression model.

```{r}
SAT_2010 <- SAT_2010 |>
  mutate(Salary = salary/1000)
SAT_plot <- ggplot(data = SAT_2010, aes(x = Salary, y = total)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  ylab("Average total score on the SAT") + 
  xlab("Average teacher salary (thousands of USD)")
SAT_plot
```

```{r}
SAT_mod1 <- lm(total ~ Salary, data = SAT_2010)
broom::tidy(SAT_mod1)
```

Lurking in the background, however, is another important factor. The percentage of students who take the SAT in each state varies dramatically (from 3% to 93% in 2010). We can create a variable called `SAT_grp` that divides the states into two groups.

```{r}
SAT_2010 |>
  skim(sat_pct)
```

```{r}
SAT_2010 <- SAT_2010 |>
  mutate(SAT_grp = ifelse(sat_pct <= 27, "Low", "High"))
SAT_2010 |>
  group_by(SAT_grp) |>
  count()
```

Now, let's visualize this with a scatterplot of these data stratified by the grouping of percentage taking the SAT.

```{r}
SAT_plot %+% SAT_2010 + 
  aes(color = SAT_grp) + 
  scale_color_brewer("% taking\nthe SAT", palette = "Set2")
```

we can derive the coefficients of the linear model fit to the two separate groups.

```{r}
SAT_2010 |>
  group_by(SAT_grp) |>
  group_modify(~broom::tidy(lm(total ~ Salary, data = .x)))
```

For each of the groups, average teacher salary is positively associated with average SAT score. But when we collapse over this variable, average teacher salary is negatively associated with average SAT score. This form of confounding is a quantitative version of [*Simpson’s paradox*](https://en.wikipedia.org/w/index.php?search=Simpson%27s%20paradox) and arises in many situations. It can be summarized in the following way:

-   Among states with a low percentage taking the SAT, teacher salaries and SAT scores are positively associated.

-   Among states with a high percentage taking the SAT, teacher salaries and SAT scores are positively associated.

-   Among all states, salaries and SAT scores are negatively associated.

Addressing confounding is straightforward if the confounding variables are measured. Stratification is one approach (as seen above). Multiple regression is another technique. Let’s add the `sat_pct` variable as an additional predictor into the regression model.

```{r}
SAT_mod2 <- lm(total ~ Salary + sat_pct, data = SAT_2010)
broom::tidy(SAT_mod2)
```

We now see that the slope for `Salary` is positive and statistically significant when we control for `sat_pct`. This is consistent with the results when the model was stratified by `SAT_grp`.

We still can’t really conclude that teacher salaries cause improvements in SAT scores. However, the associations that we observe after accounting for the confounding are likely more reliable than those that do not take those factors into account.

### Exercise 3:

For this exercise, we have Swedish Motor Insurance dataset, which was compiled by the Swedish Committee on the Analysis of Risk Premium in Motor Insurance. The data are cross-sectional, describing third party automobile insurance claims for the year 1977. The outcomes of interest are the number of claims (the frequency) and sum of payments (the severity), in Swedish kroners. Outcomes are based on 5 categories of distance driven by a vehicle, broken down by 7 geographic zones, 7 categories of recent driver claims experience and 9 types of automobile.

```{r}
# required libraries 
library(here)
library(fst)
library(tidyverse) 

# get the data 
motor_insurance <-read.csv(here("data", "SwedishMotorInsurance.csv"))
```

Let's look at the data first!

```{r}
str(motor_insurance)
# View(motor_insurance)
```

### Question 1:

Calculate adequate descriptive statistic for payment and claims in this data

**Means**:

```{r}
mean(motor_insurance$Claims)
mean(motor_insurance$Payment)
```

**correlations**

Check if there is a correlation with these two variables and plot it

```{r}
cor(motor_insurance$Claims, motor_insurance$Payment)

```

**plot**

```{r}
plot( motor_insurance$Claims, motor_insurance$Payment)

```

Let's now put a straight lines to our ploting of claims vs payments

```{r}
reg <- lm(Payment~Claims, data = motor_insurance)
plot( motor_insurance$Claims, motor_insurance$Payment)
abline(reg = reg, col="red")
```

```{r}
summary(reg)
```

Here, the straight line is defined by two things:

**intercept** : The y value at given x is zero.

**slope**: The amount of change in y when a unit of change happens in x

So, the following is our model (remember the funcitons and models)

**equation**: y= intercept + slope \* x

Thus, we can say that

payment = -3362 + 5020\*Claims

That is, every new claim results in 5020 SEK payment with an initial negative of 3362, which basically says if no claims there would be gain of 3362 for insurance companies. This model could be meaningful depending on the context. If the negative intercept makes sense in your context (i.e., the insurance company has other sources of income or factors that could lead to a gain even when there are no claims), then the model could be valid. However, if the negative intercept doesn’t make sense (i.e., it’s not possible for the insurance company to have a gain when there are no claims), you might want to reconsider the model.

## Exercise 4:

We have compiled the world record times for track events like the 100m dash and record distances for field events like the shotput into a single dataset.  This dataset includes information on the person who broke the record, his/her nationality, where the record was broken, and the year it was broken.  Note that not all world records are broken during the Olympics, with many occurring in regional or national competitions.

```{r}
wr <- read.csv(here("data", "worldrecord.csv"))
```

Let's start with eximing the data a bit

### Question 1:

How many different types of events (e.g. "Mens 100m," "Womens shotput," etc.) are represented in the dataset?

```{r}
# use table function for how many question
table(wr$Event)
```

```{r}
# or you can use count function in plyr package for better list
plyr::count(unique(wr$Event)) # instead of calling the packeag library(plyr) we used plyr::
```

### Question 2:

In what year did Usain Bolt first break the world record for the men's 100m dash?

```{r}
wr[wr$Athlete== "Usain Bolt",]
```

### Question 3:

Who was the first woman to break the women's 1 mile world record with a time of less than 260 seconds?

```{r}
wr[wr$Event=="Womens Mile",]
```

### Question 4:

Which variable tells us the record setting time or distance? What type of variable is this?

```{r}
str(wr)
```

### Question 5:

Which variable tells us when the record was broken? What type of variable is this?

```{r}
str(wr)
```

### Question 6:

For each sex, we will begin our analysis by generating a **scatterplot** of shotput distance and year. Why?

**Answer**: The scatterplot will show us if these two numeric variables are linearly related.

```{r}

# create a subset of mens shotput event
mensshotput <- wr[wr$Event=="Mens Shotput",]

# plot distance and year 
plot(mensshotput$Record, mensshotput$Year)

# create a subset of womens shotput event
womensshotput <- wr[wr$Event=="Womens Shotput",]
# plot distance and year 
plot(womensshotput$Record, womensshotput$Year)

```

### Question 7:

What will we be able to determine once we fit a **linear model** to this shotput world record data? and What is the equation for the linear model that predicts the World Record shotput distance for men?

```{r}
lm_mdl_men <- lm(Record~Year, data = mensshotput)
summary(lm_mdl_men)

# since year doesn't start with 0, recode 
mensshotput$Year <- mensshotput$Year - min(mensshotput$Year)
lm_mdl_men <- lm(Record~Year, data = mensshotput)
summary(lm_mdl_men)

# In the model, We will be able to report the average rate of change in world record shotput distance per year.
# the model now simply says that with an interceot of 17.90 every additional year effect the record 0.13 meter. 
```

### Question 9:

What is the dependent variable in our linear models?

**Answer**: Shotput distance

### Question 10:

How many records are in the menshot data frame?

```{r}
str(mensshotput)
```

### Question 11:

How many records are in the womenshot data frame?

```{r}
str(womensshotput)
```

### Question 12:

Is a linear model appropriate for the men's shotput data?

```{r}
# plot distance and year 
plot(mensshotput$Record, mensshotput$Year)
# plot distance and year 
plot(womensshotput$Record, womensshotput$Year)
```

### Question 13:

What can we say about the models for men and women?

```{r}
lm_mdl_women <- lm(Record~Year, data = womensshotput)
summary(lm_mdl_women)

# since year doesn't start with 0, recode 
womensshotput$Year <- womensshotput$Year - min(womensshotput$Year)
lm_mdl_women <- lm(Record~Year, data = womensshotput)
summary(lm_mdl_women)
```

It can be argued that given the slope of 0.23, The rate of change is greater for women than for men.

### Draw a conclusion

Based on scatterplots of the men's and women's world record shotput distance, both of these events follow a strong, **positive** linear relationship over time. The men's world record distance increases by an average of 0.13 meters per year, while the women's record distance increases by an average of 0.23 meters per year. Because the intercept estimate is the value of the record distance when **year** is equal to 0, it is not interpretable in the context of the problem. To correct this, we have **transpoze** the year variable by using the minimum year as 0. Both transpozed linear models fit the data well, with R-squared values for the men's and women's models equal to 17.9 and 14.83, respectively. Overall, the rate of change record per unit increase in year is greater for women than for men.

## Exercise 5:

Keep on working with World Record Data set. Now, our **Reserach Questions** is "How have world record times for the men's and women's mile event changed over the years?"

Yet, let's start with some basic of linear regression!

### Question 1:

When fitting a model to data, what should you do **first** to examine the data?

**Answer**: Create a scatterplot of the two variables of interest. 

### Question 2:

When fitting a linear model, what will tell you the **proportion of variance** in the dependent variable that can be explained by the independent variable?

**Answer**: the R-squared value

### Question 3:

Which scatterplot shows a **stronger** linear relationship between World Record times in the Mile and Year:

```{r}
# create a subset of mens mile event
mensmile <- wr[wr$Event=="Mens Mile",]

# plot mile and year 
plot(mensmile$Record, mensmile$Year)

# create a subset of womens mile event
womensmile <- wr[wr$Event=="Womens Mile",]
# plot mile and year 
plot(womensmile$Record, womensmile$Year)
```

### Question 4:

On average, how many *seconds* do men trim off the world record time in the Mile each year? 

```{r}
men_mdl_mile <- lm(Record~Year, data = mensmile)
summary(men_mdl_mile)
```

### Question 5:

On average, how many *seconds* do women trim off the world record time in the Mile each year?

```{r}
women_mdl_mile <- lm(Record~Year, data = womensmile)
summary(women_mdl_mile)
```

### Question 6:

How many **years** would you predict it would take for the men's mile record to decrease by one full second? Use the model equation to help you answer the question.

```{r}
# model equation record = 1007.47 + (-0.393)*Year
1/0.393
```

### Question 7:

How many **years** would you predict it would take for the women's mile record to decrease by one full second? Use the model equation to help you answer the question.

```{r}
# model equation record = 2189.2 + (-0.972)*Year
1/0.972
```

### Question 8:

What proportion of variance in the men's and women's World Record times in the Mile can be explained by year? 

```         
Adjusted R-squared:  0.9767 # for men
Adjusted R-squared:  0.8861 # for women 
```

### Question 9:

Is the following is a reasonable conclusion to draw from this analysis?

World record times in the Mile have decreased linearly over the last several decades for both men and women.

**Answer**: YES! We can claim that World record times in the Mile have decreased linearly over the last several decades for both men and women.

### **Draw a conclusion**

Based on scatterplots of the men's and women's world record mile event, both of these events follow a strong, **negative** relationship over time. For both groups, the assumption of linearity appears to be satisﬁed. The men's world record mile time decreases by an average of 0.393 seconds per year, while the women's record distance decreases by an average of 0.976 seconds per year. Because the intercept estimate is the value of the record time when year is equal to 0, it is not interpretable in the context of the problem. Both linear models ﬁt the data well, with R-squared values for the men's and women's models equal to 0.976 and 0.886 , respectively. For the men's world record, 97.7% of the correct is explained by the linear model of year, while for the female world record, 88.6% of the correct in performance can be explained by the linear model of year.

## Exercise 6:

Keep on working with World Record Data set. Now, **We want to find the best-fitting linear model for men's pole vault world records since 1970.**

To do that, first, let's create a new data frame that contains the world record cases in the men's pole vault event in years 1970 and later. 

```{r}
menspole <- wr[wr$Event=="Mens Polevault" & wr$Year>= 1970,]
```

### Question 1:

What is the standing world record height (in meters) for men's pole vault?

```{r}
max(menspole$Record)
```

### Question 2:

In what year did the pole vault record first exceed **6 meters**?

```{r}
menspole[menspole$Record>6,]
```

### Question 3:

Let's now Create a scatterplot showing the men's pole vault records since 1970 as a function of year. Fit a linear model to the data.

```{r}
men_mdl_pole <- lm(Record ~Year, data = menspole)
plot(menspole$Year, menspole$Record)
abline(reg = men_mdl_pole, col="red")
```

### Question 4:

Is the following best describes how the record has changed over time?

**The record pole vault height steadily increases over time.**

**Answer**: Yes, indeed, the record pole vault height steadily increases over time.

### Question 5:

Report the coefficient estimates for the linear model that describes the change in the men's pole vault world record since 1970

```{r}
summary(men_mdl_pole)
```

What is the intercept and slope?

intercept = 51.854

Slope = 0.0291

### Question 6:

What best describes how the men's pole vault world record has changed since 1970?

**Answer**: The record has increased by an average of 0.03 meters per year since 1970.

# Categorical Explanatory Variables

Categorical variables require special attention in regression analysis because, unlike dichotomous or continuous variables, they cannot by entered into the regression equation just as they are.  Instead, they need to be recoded into a series of variables which can then be entered into the regression model.  There are a variety of coding systems that can be used when recoding categorical variables.  Regardless of the coding system you choose, the overall effect of the categorical variable will remain the same. Ideally, you would choose a coding system that reflects the comparisons that you want to make.  For example, you may want to compare each level of the categorical variable to the lowest level (or any given level).  In that case you would use a system called **simple coding**.  Or you may want to compare each level to the next higher level, in which case you would want to use **repeated coding**.  By deliberately choosing a coding system, you can obtain comparisons that are most meaningful for testing your hypotheses. Below is a table listing various types of contrasts and the comparison that they make. **We should note that some forms of coding make more sense with ordinal categorical variables than with nominal categorical variables!**

## Exercise 1:

Let's get some data to work on categorical variable and regression! We have here a fish dataset, which records of 7 common different fish species in fish market sales. With this dataset, a predictive model can be performed using machine friendly data and estimate the weight of fish can be predicted.

```{r}
fish <- read.csv(here("data", "Fish.csv"))
str(fish)

```

For convenience, we have subset data for only four species!

```{r}
species <-c("Bream", "Perch", "Pike", "Roach")
fish <- fish[fish$Species==species,]
```

### Question 1:

Create a distribution plot for this dataset!

```{r}
barplot(table(fish$Species))
```

::: callout-tip
👋 A better way to visualize this could be using ggplot faceting options

```{r}
ggplot(data = fish, aes(x=Weight)) + 
  geom_histogram( bins = 9) +
  facet_wrap(~Species)
```
:::

### Question 2:

What are the basic descriptive metrics (mean)

```{r}
# in order to calculate means for each group we use aggregate() in base R
aggregate(fish$Weight, list(fish$Species), FUN=mean)
```

::: callout-tip
::: callout-tip
👋 A better way to visualize this could be using group_by() in dplyr pacakge

```{r}
fish |> 
  group_by(Species) |> 
  summarise(mean_wight = mean(Weight))
```
:::
:::

### Question 3:

Report the coefficient estimates for the linear model that describes the change in the weight for different fish species

```{r}
lm(formula = Weight~Species, data = fish)
```

### Question 4:

What is the problem with this model?

**Answer**: The problem with this model is that we have 4 spices and change in the weight of one of the species (Bream) has been repoted as intercept value, which is not correct. The model also has negative values for Persch and Roach, which is uninterpretable! Therefore, we have to set linear model with a zero intercept

```{r}
# with no intercept
lm(formula = Weight ~Species+0, data = fish)
```

Now, it is much more readable. We can say that according to our fish data, the predicted weights are 596, 372, 730, and 130 for bream, perch, pike, and roach species.

## Exercise 2

Regression lets you predict the values of a response variable from known values of explanatory variables. Which variable you use as the response variable depends on the question you are trying to answer, but in many datasets there will be an obvious choice for variables that would be interesting to predict. Over the next few exercises, you'll explore a Taiwan real estate dataset with 4 variables, which is a part of a market historical data set of real estate valuation collected from Sindian Dist., New Taipei City, Taiwan. The real estate valuation is the regression problem.

```{r}
taiwan_real_estate <- read.fst(here("data", "taiwan_real_estate.fst"))
str(taiwan_real_estate)
```

### Question 1:

Type `View(taiwan_real_estate)` in the console to view the dataset, and decide which variable would make a good response variable.

```{r}
View(taiwan_real_estate)
```

**Answer**: Predicting prices is a common business task, so house price makes a good response variable. So, price_twd_msq variable is one of the possible response variable for this dataset.

### Question 2:

Before you can run any statistical models, it's usually a good idea to visualize your dataset. Here, we'll look at the relationship between house price per area and the number of nearby convenience stores, using the Taiwan real estate dataset.

```{r}
plot(taiwan_real_estate$price_twd_msq, taiwan_real_estate$n_convenience)
```

👋 One challenge in this dataset is that the number of convenience stores contains integer data, causing points to overlap. To solve this, you will make the points transparent. This could be best done by using ggplot

```{r}
ggplot(data = taiwan_real_estate, aes(x=n_convenience, y= price_twd_msq))+
  geom_point(alpha=0.5)+
  geom_smooth(method="lm", se=FALSE) +
  scale_x_continuous("No. of convenience stores şn walking distance")+
  scale_y_continuous("House price per unit area in Taiqan dollars per square meter")+
  ggtitle("Taiwan Real Estate Price: Conveniency effects on Prices")+
  theme_bw()
```

### Question 3:

Linear regression models always fit a straight line to the data. Straight lines are defined by two properties: their intercept and their slope.

Here, you can see a scatter plot of house price per area versus number of nearby convenience stores, using the Taiwan real estate dataset.

Also run a linear regression with `price_twd_msq` as the response variable, `n_convenience` as the explanatory variable, and `taiwan_real_estate`as the dataset.

```{r}
taiwan_model <- lm(price_twd_msq~n_convenience, data = taiwan_real_estate)
plot(price_twd_msq~n_convenience, data = taiwan_real_estate)
abline(reg = taiwan_model, col="red")
```

### 

💡Remember that

The intercept is the y-value when x equals zero.

The slope is the rate of change in the y direction divided by the rate of change in the x direction.

```{r}
summary(taiwan_model)
```

### Question 4:

The model had an `(Intercept)` coefficient of `8.2242`. What does this mean?

**Answers:** On average, a house with zero convenience stores nearby had a price of 8.2242 TWD per square meter.

### Question 5:

The model had an `n_convenience` coefficient of `0.7981`. What does this mean?

**Answers:** If you increase the number of nearby convenience stores by one, then the expected increase in house price is `0.7981` TWD per square meter.

## Exercise 3

If the explanatory variable is categorical, the scatter plot that you used before to visualize the data doesn't make sense. Instead, a good option is to draw a barplot or histogram for each category.

The Taiwan real estate dataset has a categorical variable in the form of the age of each house. The ages have been split into 3 groups: 0 to 15 years, 15 to 30 years, and 30 to 45 years.

### Question 1:

Using `taiwan_real_estate`, plot a histogram of `price_twd_msq` 

```{r}
barplot(table(taiwan_real_estate$house_age_years))
```

👋 A better way of visualising would be the use of ggplot faceting option

```{r}
ggplot(data = taiwan_real_estate, aes(x=price_twd_msq))+
  geom_histogram(bins = 10) +
  facet_wrap(~house_age_years)
```

### Question 2:

Group `taiwan_real_estate` by `house_age_years`.

```{r}
table(taiwan_real_estate$house_age_years)
```

### Question 3:

Calculate the mean `price_twd_msq` for each group

```{r}
group1 <-taiwan_real_estate[taiwan_real_estate$house_age_years=="0 to 15",]
mean(group1$price_twd_msq, na.rm=TRUE)

group2 <-taiwan_real_estate[taiwan_real_estate$house_age_years=="15 to 30",]
mean(group2$price_twd_msq, na.rm=TRUE)

group3 <-taiwan_real_estate[taiwan_real_estate$house_age_years=="30 to 45",]
mean(group3$price_twd_msq, na.rm=TRUE)
```

👋 Much better and much easier way is to use dplyr group by and summarise functions

```{r}
taiwan_real_estate |> 
  group_by(house_age_years) |> 
  summarise(mean_by_group =mean(price_twd_msq))
```

### Question 4:

Run a linear regression with `price_twd_msq` as the response variable, `house_age_years` as the explanatory variable, and `taiwan_real_estate` as the dataset. Assign to `mdl_price_vs_age`.

```{r}
mdl_price_vs_age <- lm( price_twd_msq ~ house_age_years+0, data = taiwan_real_estate)
summary(mdl_price_vs_age)
```

## Draw a conlucion

We can argue that on average house prices in Taiwan has some correlations with the age of the house. 0-15 age houses are on average cost 12.635 , while 15 to 30 years cost 9.987. Interestingly, older than 30 years houses, even more expensive that middle age houses. This requires more attention, and why these houses are expensive. One explanation could be that these older houses are relatively larger than middle age and new houses, and size could be a better estimator in estimating houses prices in Taiwan real estate market.
