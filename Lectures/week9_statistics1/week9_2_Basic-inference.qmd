---
title: "Mechanism of Inference"
author: "Hakan Mehmetcik"
format: pdf
editor: visual
---

## Introduction

After having a summary of the data via descriptive statistics, we would answer questions using statistical inference (inferenrial statistics). In fact, descriptive statistics is one of the smallest parts of statistics, and one of the least powerful. The bigger and more useful part of statistics is that it provides that let you make inferences about data. **Inference means making statements about the population based on a sample.**

### How valid our inference?

A polling company conducts surveys, usually a pretty big one because they can afford it. We assume that the data collected by the polling companies is pretty representative of the population at large. But how representative?

**Inferential statistics** provides the tools that we need to answer these sorts of questions, and since these kinds of questions lie at the heart of the scientific enterprise, they take up the lions share of every introductory course on statistics and research methods.

To start with, we need a basic understanding of probability since most of the time we deal with "what is the chance" type of questions.

## Probability Theory

Probability theory is the doctrine of chances. As such, it tells you how often different kinds of events will happen.

For example, all of these questions are things you can answer using probability theory:

• What are the chances of a fair coin coming up heads 10 times in a row?

• If I roll two six sided dice, how likely is it that I'll roll two sixes?

• How likely is it that five cards drawn from a perfectly shuffled deck will all be hearts?

• What are the chances that I'll win the lottery?

::: callout-caution
💡Note that in each case, the true value of the world is known, and the question relates to the what kinds of events will happen! In the first question I know that the coin is fair, so there's a 50% chance that any individual coin flip will come up heads. In the second question, I know that the chance of rolling a 6 on a single die is 1 in 6. In the third question I know that the deck is shuffled properly. And in the fourth question, I know that the lottery follows specific rules. You get the idea! right?
:::

The underlying model can be quite simple. For instance, in the coin flipping example, we can write down the model like this:

P (heads) = 0.5, which you can read as "the probability of heads is 0.5.

In probability theory, the model is known, but the data are not. That is, when you flip the coin you do know, on average, what you get, but your data could be 3 heads, 6 heads, 8 heads per ten flip!

Statistics speak from the data, while probability speaks from the the value of the world. That is to say statistics answer question about the sample data in order to make inference about the true value relying on the knowledge of the world!

## What is the role of probability

In statistics, we rely on **frequentist probability model**, which basically argues that if we repeat a random phenomenon a large number of times, it will get closer to the true probability of event occurring.

That is to say, if you flip a coin enough, you will get 0.5 head or tail!

This is called as the **Law of the Large Numbers**. Statistically speaking, as the observation number increase, both the sample mean and the true value of the proportion p will converge to the population mean and proportion.

**A repeated claim:** Probability basically helps us measure uncertainty. In an other words, how confident we are in the conclusions that we draw from inference.

### The frequentist view

f you flip a fair coin over and over again, the proportion of heads that you've seen eventually settles down, and converges to the true probability of 0.5.

![](images/Screenshot%202022-12-06%20at%2013.12.25.png)

### The Bayesian View

The most common way of thinking about subjective probability is to define the probability of an event as the degree of belief that an intelligent and rational agent assigns to that truth of that event. We will come to this Bayesian statistic with a detailed class in the future!

## Calculating Probability and the axioms of probability

In order to calculate probability, we have to know full list of all possible outcomes for a random phenomenon. This list is called as **sample space,** denoted with S.

Flip a coin, S = {H,T}

flip a coin 2 times, S= {HH, HT, TH, TT}

**An event** any subset of sample space and noted with the capital letter like A, B, C, etc.

an event of having one tail when you flip a coin twice, A={HT, TH}

then, the probability of an event A occurring is the number of outcomes in A divided by the numbers of outcomes in S

That is, P(A) = #outcomes in A / #outcomes in S

an event of having one tail when you flip a coin twice, P(A)= 2/4 (1/2)

An event not occurring called as **complement**. Thus, complement of A, shown as A\^c

not having A P(A\^c)= 1-1/2

### Some basic rules

1.  P(A) \> 0

2.  P(S) = 1

3.  if A and B disjoint events, then P(A or B) = P(A) + P(B)

    What is the probability that a card chosen from a standard deck will be a Jack or a heart?\
    Solution:

    -   p(Jack) = 4/52 

    -   p(Heart) = 13/52 

    -   p(Jack of Hearts) = 1/52

    So:\
    p(Jack or Heart) = p(Jack) + p(Heart) -- p(Jack of Hearts) = 4/52 + 13/52 -- 1/52 = 16/52.

4.  (**independent** events): p(A and B) = p(A) \* p(B).

    For inst: The odds of it raining today is 40%; the odds of you getting a hole in one in golf are 0.08%. What are your odds of it raining and you getting a hole in one?

5.  (**dependent** events): p(A and B) = p(A) \* p(B\|A)

    for inst: You have 52 candidates for a committee. Four are persons aged 18 to 21. If you randomly select one person, and then (without replacing the first person's name), randomly select a second person, what is the probability both people will be between 18 and 21 years old?

    **Solution**:\
    Step 1: Figure out the probability of choosing an 18 to 21 year old on the first draw. As there are 52 possibilities, and 4 are aged 18 to 21, you have a 4/52 = 1/13 chance.

    Step 2: Figure out p(B\|A), which is the probability of the next event (choosing a second person aged 18 to 21) **given that the first event in Step 1 has already happened**.\
    There are 51 people left, and only 3 are aged 18 to 21 now, so the probability of choosing a young adult again is 3/51 = 1 / 17.

    Step 3: Multiply your probabilities from Step 1(p(A)) and Step 2(p(B\|A)) together:\
    p(A) \* p(B\|A) = 1/13 \* 1/17 = 1/221.

## Random Variables and Probability Distributions

Until now, we only dealt with categorical variables (head or tail - Tru or False). how about numeric variables? When we speak of numeric variables, we are actually speaking of a **random variable**, which is a numeric summary of a random phenomenon.

**A discrete random variable** is one that has countable numbers of outcomes, while **continiues random variable** is a random varibale with an uncountable numbers of outcomes.

Examples of discrete random variables include **the number of children in a family**, the Friday night attendance at a cinema, the number of patients in a doctor's surgery, the number of defective light bulbs in a box of ten.

the **mass, temperature, energy, speed, length**, and so on are all examples of continuous variables.

**A probability Distribution** is a summary of a random variable that gives all possible values of the random variable along with their probability of occurring.

![](images/Discrete-and-continuous-random-variables.png)

So for a quick wrap-up:

::: callout-note
💡The key characteristics and points to understand about random variables:

1.  **Definition**: A random variable is a variable whose value is determined by the outcome of a random experiment. It associates a numerical value with each possible outcome of the experiment.

2.  **Notation**: Random variables are typically denoted using capital letters, such as "X," "Y," or "Z." For example, if we have a random variable representing the outcome of rolling a six-sided die, we might denote it as "X."

3.  **Types of Random Variables**:

    -   **Discrete Random Variable**: Takes on a countable number of distinct values. Examples include the number of students attending in Statistic Class

    -   **Continuous Random Variable**: Can take any value within a specified range. Examples include measurements like height, weight, or time.

4.  **Probability Distribution**: Each random variable has an associated probability distribution, which describes how likely each possible value is to occur. This distribution can be represented in various ways, such as a probability mass function (PMF) for discrete variables or a probability density function (PDF) for continuous variables.

5.  **Mean and Variance**: Random variables have a mean (expected value) and a variance that provide important information about their central tendency and variability.

6.  **Applications**: Random variables are used to model real-world phenomena with inherent uncertainty. They are central to statistical analysis, hypothesis testing, and making predictions.

7.  **Examples**:

    -   Discrete Random Variable Example: The number of heads obtained when flipping a coin three times can be represented as a discrete random variable.

    -   Continuous Random Variable Example: The time it takes for a computer to execute a task can be represented as a continuous random variable.
:::

```{r}
blue_jeans <-0.5 
grey_jeans <- 0.3 
green_jeans <- 0.1 
black_suit <- 0 
blue_suit <- 0.1  
gardrope <- rbind(blue_jeans,grey_jeans,green_jeans,black_suit,blue_suit) 
```

```{r}
barplot(gardrope, beside = T, names.arg = row.names(gardrope))
```

Explanation:

1.  **Define Probabilities:**

    -   **`blue_jeans`**, **`grey_jeans`**, **`green_jeans`**, **`black_suit`**, and **`blue_suit`** represent the probabilities of selecting each item of clothing from a wardrobe. These probabilities are specified as percentages.

2.  **Create a Matrix:**

    -   **`rbind`** is used to combine these probabilities into a matrix called **`gardrope`**.

    -   Each row of the matrix corresponds to a different item of clothing, and each column corresponds to a different wardrobe category.

3.  **Matrix Structure:**

    -   The resulting matrix looks like this:

        ```{r}
        gardrope
        ```

-   The column header **`[,1]`** indicates that there is one column in the matrix.

-   The rows represent different clothing items, and the values in the matrix represent the probabilities of selecting each item.

there's an enormous range of distributions out there. However,we rely on five distributions: the binomial distribution, the normal distribution, the t distribution, the χ2 ("chi-square") distribution and the F distribution.

## The Binomial Distribution

Say that you were to flip a coin once. In this case, there are 2 possibilities: H or T. So, do this 10 times, the counts of H or tails will be your probability variable.

For a binomial distribution, there are two parameters:

-   the number of repeated trials n

-   the probability success in an individual trial p

So, our X \~ Bin(10, 0.5) number of success is n x p (10x0.5=5).

We have to chacke If the observed result is higher or lower than what we expect.

R has a function called dbinom() that calculates binomial probabilities for us.

-   x\. This is a number, or vector of numbers, specifying the outcomes whose probability you're trying to calculate.

• size. This is a number telling R the size of the experiment.

• prob. This is the success probability for any one trial in the experiment.

```{r}
# getting 4 times 1 from a die with 20 trial
dbinom(4, size = 20, prob = 1/6)

# getting 5 head from flipping a coin 20 times
dbinom(5, 20, 0.5)
```

![](images/Screenshot%202022-12-06%20at%2022.23.51.png)

![](images/Screenshot%202022-12-06%20at%2022.24.56.png)

I'm rolling 20 dice, and each die has a 1 in 6 chance of coming up skulls. Suppose, however, that I want to know the probability of rolling 4 or fewer skulls. If I wanted to, I could use the dbinom() function to calculate the exact probability of rolling 0 skulls, 1 skull, 2 skulls, 3 skulls and 4 skulls and then add these up, but there's a faster way. Instead, I can calculate this using the pbinom() function.

```{r}
pbinom(q=4, size=20, prob=1/6)
```

The result suggest that there is a 76.9% chance that I will roll 4 or fewer skulls.

Let's say I want to calculate the 75th percentile of the binomial distribution. If we're sticking with our skulls example:

```{r}
qbinom(p=0.75, size=20, prob=1/6)
```

this result is telling us is that the 75th percentile of the binomial distribution is 4!

To use the rbinom() function, you specify how many times R should "simulate" the experiment using the n argument, and it will generate random outcomes from the binomial distribution. So, for instance, suppose I were to repeat my die rolling experiment 100 times. I could get R to simulate the results of these experiments by using the following command:

```{r}
rbinom(n=100, size = 20, prob = 1/6)
```

## The Normal Distribution

Normal distribution is one of the most important distribution in statistics. Normal distribution is known as "the bell curve" or a "Gaussian distribution" too.

The normal distribution is symmetric, uni modal, and it is a distribution for numeric continuous variable.

In normal distribution there are two parameters:

-   the parameter mean (defines the center)

-   the variance (or sd) (defines the spread)

So, X \~Normal(u,o)

Just like binomial, the R functions for the normal distribution are dnorm(), pnorm(), qnorm() and rnorm(). the argument names for the parameters are mean and sd. In pretty much every other respect, there's nothing else to add. And, don't forget the normal distribution is continuous, whereas the binomial is discrete (For instance, in the die rolling example from the last section, it was possible to get 3 skulls or 4 skulls, but impossible to get 3.9 skulls)

The area under the curve tells you the probability that an observation falls within a particular range. The solid lines plot normal distributions with mean μ " 0 and standard deviation σ " 1. The shaded areas illustrate "areas under the curve" for two important cases. In panel a, we can see that there is a 68.3% chance that an observation will fall within one standard deviation of the mean. In panel b, we see that there is a 95.4% chance that an observation will fall within two standard deviations of the mean.

![](images/Screenshot%202022-12-06%20at%2022.41.15.png)

![](images/Screenshot%202022-12-06%20at%2022.41.44.png)

To find out the probability associated with a particular range, what you need to do is calculate the "area under the curve"

```{r}
# generate 1000 normally distributed observation
normal.a <- rnorm(1000, mean=0, sd=1)
normal.a
mean(normal.a)
sd(normal.a)
```

So the normal.a variable contains 1000 numbers that are normally distributed, having mean =0 and sd=1.

```{r}
hist(normal.a)
```

## Sampling Distributions

In almost every situation of interest, what we have available to us as researchers is a sample of data. We might have run experiment with some number of participants; a polling company might have phoned some number of people to ask questions about voting intentions; etc. Regardless: the data set available to us is finite, and incomplete.

Statistics are numbers that are calculated from sample and that generally speaking estimate parameters. For exm, sample mean x(\^) estimates the population mean u, sample proportion p(\^) estimates the population proportion.

What kinds of things would we like to learn about? And how do we learn them? These are the questions that lie at the heart of inferential statistics, and they are traditionally divided into two "big ideas": estimation and hypothesis testing.

### Sampling Procedure

**Simple Random sample**: Selecting black or white chips from a box without replacement

![![](images/Screenshot%202022-12-06%20at%2023.06.46.png)](images/Screenshot%202022-12-06%20at%2023.04.00.png)

::: callout-note
⚠️Most samples are not simple random sample! More generally though, it's important to remember that random sampling is a means to an end, not the end in itself.
:::

![](images/Screenshot%202022-12-06%20at%2023.09.05.png)

Lets make fake IQ scores

```{r}
IQ <- rnorm(n=1000, mean=100, sd=15) # generate IQ scores
IQ <- round(IQ) # IQs are whole numbers

summary(IQ)
```

```{r}
IQ.five1 <- round(rnorm(5, mean = 100, sd=15))
summary(IQ.five1)
```

```{r}
#let's replicate
IQ.five2 <- round(rnorm(5, mean = 100, sd=15))
IQ.five3 <- round(rnorm(5, mean = 100, sd=15))
IQ.five4 <- round(rnorm(5, mean = 100, sd=15))
IQ.five5 <- round(rnorm(5, mean = 100, sd=15))

replication <- as.data.frame(rbind(IQ.five1, IQ.five2, IQ.five3, IQ.five4, IQ.five5))
replication$mean <-  apply(replication[,-1], 1, mean) # add rowwise mean 
replication
```

**The Central limit theorem:** the bigger the sample size, the narrower the sampling distribution gets.

```{r}
hist(IQ.five1)
hist(IQ)
```

::: callout-note
💡no matter what shape your population distribution is, as N increases the sampling distribution of the mean starts to look more like a normal distribution!
:::

Thus, Central Limit theorem basically suggest that:

• The mean of the sampling distribution is the same as the mean of the population

• The standard deviation of the sampling distribution (i.e., the standard error) gets smaller as the

sample size increases

• The shape of the sampling distribution becomes normal as the sample size increases.

![](images/Screenshot%202022-12-13%20at%2020.10.53.png)

## Estimating Population Parameters

It is all about the procedure for estimating population parameters from a sample of data. How to do that?

![](images/1_6.jpg)

Don't forget that a sample statistic is a description of your data, whereas the estimate is a guess about the population.

### The mean of the sampling distribution is the same as the mean of the population

First guess is about the estimation of population mean. This exactly equals to sample mean. As simple as it is!

![](images/Screenshot%202022-12-13%20at%2020.17.01.png)

```{r}
IQ.2 <- round(rnorm(10000, mean = 100, sd=15))
sample1 <- sample(IQ.2,10)
mean1 <- mean(sample1)
sample2 <- sample(IQ.2,10)
mean2 <- mean(sample2)
sample3 <- sample(IQ.2,10)
mean3 <- mean(sample3)
sample4 <- sample(IQ.2,10)
mean4 <- mean(sample4)
sample5 <- sample(IQ.2,10)
mean5 <- mean(sample5)
sample6 <- sample(IQ.2,10)
mean6 <- mean(sample6)
sample7 <- sample(IQ.2,10)
mean7 <- mean(sample7)
sample8 <- sample(IQ.2,10)
mean8 <- mean(sample8)
sample9 <- sample(IQ.2,10)
mean9 <- mean(sample9)
sample10 <- sample(IQ.2,10)
mean10 <- mean(sample10)
sample11 <- sample(IQ.2,10)
mean11 <- mean(sample11)
sample12 <- sample(IQ.2,10)
mean12 <- mean(sample12)
sample13 <- sample(IQ.2,10)
mean13 <- mean(sample13)
sample14 <- sample(IQ.2,10)
mean14 <- mean(sample14)
sample15 <- sample(IQ.2,10)
mean15 <- mean(sample15)
sample16 <- sample(IQ.2,10)
mean16 <- mean(sample16)
sample17 <- sample(IQ.2,10)
mean17 <- mean(sample17)
sample18 <- sample(IQ.2,10)
mean18 <- mean(sample18)
sample19 <- sample(IQ.2,10)
mean19 <- mean(sample19)
sample20 <- sample(IQ.2,10)
mean20 <- mean(sample20)
sample21 <- sample(IQ.2,10)
mean21 <- mean(sample21)
sample22 <- sample(IQ.2,10)
mean22 <- mean(sample22)
sample23 <- sample(IQ.2,10)
mean23 <- mean(sample23)
sample24 <- sample(IQ.2,10)
mean24 <- mean(sample24)
sample25 <- sample(IQ.2,10)
mean25 <- mean(sample25)
sample26 <- sample(IQ.2,10)
mean26 <- mean(sample26)
sample27 <- sample(IQ.2,10)
mean27 <- mean(sample27)
sample28 <- sample(IQ.2,10)
mean28 <- mean(sample28)
sample29 <- sample(IQ.2,10)
mean29 <- mean(sample29)
sample30 <- sample(IQ.2,10)
mean30 <- mean(sample30)

(mean1+mean2+mean3+mean4+mean5+mean6+mean7+mean8+mean9+mean10+mean11+mean12+mean13+mean14+mean15+mean16+mean17+mean18+mean19+mean20+mean21+mean22+mean23+mean24+mean25+mean26+mean27+mean28+mean29+mean30)/30
```

![](images/Screenshot%202022-12-13%20at%2020.43.36.png)

Central Limit Theorem only works:

1.  Random Sampling
2.  Normal Distribution
    1.  if not, take att least 30 sample!
3.  Independence condition
    1.  sampling with replacement , then we are ok!

    2.  sampling without replacement (the 10% **rule!)**

## Estimating a confidence Interval

The thing that has been missing from this discussion is an attempt to quantify the amount of uncertainty that attaches to our estimate. It's not enough to be able guess that, say, the mean IQ of undergraduate psychology students is 115 (yes, I just made that number up). We also want to be able to say something that expresses the degree of certainty that we have in our guess. For example, it would be nice to be able to say that there is a 95% chance that the true mean lies between 109 and 121. The name for this is a confidence interval for the mean.
