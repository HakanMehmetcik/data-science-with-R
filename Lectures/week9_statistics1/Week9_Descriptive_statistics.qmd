---
title: "Descriptive_statistics"
author: "Hakan Mehmetcik"
format: pdf
editor: visual
execute: 
  echo: true
  warning: true
  output: asis
df-print: kable
---

```{r}
here <- here::here()
dpath <- "data"
```

# Week 3: Descriptive Statistics

## **Learning Objectives:**

By the end of this chapter, students should be able to:

1.  **Calculate Measures of Central Location: Mean and Median:**

    -   Define the mean (average) and median as measures of central location.

    -   Calculate the mean and median of a dataset.

    -   Recognize situations where each measure is more appropriate for summarizing data.

2.  **Calculate Measures of Spread: Variance, Standard Deviation, and Inter-Quartile Range:**

    -   Define variance, standard deviation, and inter-quartile range (IQR) as measures of data spread or dispersion.

    -   Calculate variance, standard deviation, and IQR for a dataset.

    -   Interpret these measures to understand the variability in data.

3.  **Identify Outlier:**

    -   Define outlier as values that deviate significantly from the rest of the data distribution.

    -   Recognize various methods, such as the IQR method, for identifying outlier in a dataset.

    -   Understand the potential impact of outlier on data analysis and interpretation.

Here is a quick falshback: Different data types and the possible visual tools to represent them:

| **Data Type**   | **Visual Tool**      | **Description**                                               |
|-----------------|----------------------|---------------------------------------------------------------|
| Categorical     | Bar Plot             | Displays the frequency or count of each category.             |
| Categorical     | Pie Chart            | Shows the proportions or percentages of each category.        |
| Categorical     | Stacked Bar Plot     | Compares the distribution of subcategories within categories. |
| Numeric         | Histogram            | Represents the distribution of numeric values in intervals.   |
| Numeric         | Box Plot             | Displays the summary statistics and identifies outliers.      |
| Numeric         | Density Plot         | Shows the probability density function of the data.           |
| Numeric         | Scatter Plot         | Depicts the relationship between two numeric variables.       |
| Time Series     | Time Series Plot     | Visualizes data collected over time, such as stock prices.    |
| Geographic      | Scatter Plot on Maps | Maps data points with geographic coordinates.                 |
| Textual/Nominal | Word Cloud           | Highlights word frequency in textual data.                    |

These are some common data types and the corresponding visualization tools frequently used in data analysis. The choice of visualization method depends on the nature of your data and the insights you want to convey.

## Introduction to Descriptive Statistics

Let's recall that statistics is to help us answer questions from the data. Population and parameter as information about the population is generally unknown. In order to estimate these parameters, we need our sample, whether from a designed experiment or observational study. But just like we cannot use entire sample, we have to rely on appropriately chosen summary number of the sample to estimate our parameters. This is statistic, or any number calculated from our sample.

**To begin with, you have to find a way to summarize the data, that is what descriptive statistics is all about.** Further, we cannot---or should not---make definitive decisions from descriptive statistics. Rather, descriptive statistics can give us a general idea what is going on in the dataset and point us in the direction of interesting relationships between variables in the dataset.

## Descriptive Statistics with Categorical Variables

When it comes to categorical variables, we are interested in percentage of population or sample. The population proportion (notated p) or percentage of the population that falls into same category of interests.

p = number of subjects in category / sample size

Here, the most common summary is the frequency table, which gives the counts for the number of subjects that fall in each category.

You can create a frequency table in R using the **`table()`** function. In your case, you have a vector **`books_readers`** that contains categories of readers and their corresponding counts. Here's the R code to create a frequency table and an explanation of how to interpret it:

```{r}
# Create the vector with reader categories and counts
books_readers <- c("no_books"=395, "print_only"=577, "digital_only"=91, "print_and_digital"=425)

# Create a frequency table
frequency_table <- table(books_readers)

# Print the frequency table
print(frequency_table)

```

Now, let's interpret the frequency table:

-   "no_books": 395 individuals fall into the category of "no_books." This means that 395 readers do not read any books (print or digital).

-   "print_only": 577 individuals fall into the category of "print_only." This means that 577 readers exclusively read print books and do not read digital books.

-   "digital_only": 91 individuals fall into the category of "digital_only." This means that 91 readers exclusively read digital books and do not read print books.

-   "print_and_digital": 425 individuals fall into the category of "print_and_digital." This means that 425 readers read both print and digital books.

In summary, the frequency table breaks down the count of readers into different categories based on their reading habits. It provides a clear and concise summary of how many readers fall into each category. This information can be useful for analyzing and understanding the distribution of reading preferences among the surveyed individuals.

Ok, Now let's look into what would be the sample proportion for people who read only print books?

```{r}
# Count of "print_only"
count_print_only <- books_readers["print_only"]

# Total sample size (sum of all counts)
total_sample_size <- sum(books_readers)

# Calculate the sample proportion
sample_proportion_print_only <- count_print_only / total_sample_size

# Print the sample proportion
print(sample_proportion_print_only)

```

A contingency tables lists all of the possible variations between two or more categorical variables!

```{r}
# Calculate proportions for each category
proportions <- books_readers / sum(books_readers)

# Print the proportions
print(proportions)

```

However, in R, we use prob.table() function to create such frequency table, it is the short cut and most effective way to create and analyze. It will calculate the proportions for each category in the "books_readers" vector by dividing each count by the total sample size. It provides the proportions of readers in each category relative to the entire sample

```{r}
prop.table(books_readers)
```

Are you an active, casual, or uninterested consumer of science news?

```{r}
# Create vectors for each racial/ethnic group
white <- c("active"=487, "causal"=916, "uninterested"=1431, "no_answer"=28)
black <- c("active"=59, "causal"=98, "uninterested"=227, "no_answer"=8)
hispanic <- c("active"=89, "causal"=152, "uninterested"=183, "no_answer"=23)

# Combine the vectors into a data frame
my_table <- as.data.frame(rbind(white, black, hispanic))

# Print the data frame
print(my_table)
```

This code creates a data frame **`my_table`** where each row corresponds to a racial/ethnic group (white, black, and hispanic), and each column corresponds to a category within that group (active, causal, uninterested, and no_answer). The numbers in the data frame represent the counts of individuals in each category for each group.

### A contingency table: Descriptive statistics for more than one-categorical variables

You can now perform various analyses and calculations using this data frame, such as calculating proportions, conducting chi-squared tests, or creating visualizations to explore the relationships between racial/ethnic groups and their preferences or responses in different categories.

```{r}
prop.table(my_table)
```

Let's break down what each line of code does:

1.  This line calculates the row sums for each row in the data frame and stores the result in a new column called "rowsum" in **`my_table`**. Each element in the "rowsum" column represents the sum of counts for each racial/ethnic group.

2.  This line calculates the column sums for each column in the data frame and adds a new row labeled "colsum" to **`my_table`**. Each element in the "colsum" row represents the sum of counts for each category across all racial/ethnic groups.

With the help of pro.table() function, **`my_table`** data frame will include row sums in the "rowsum" column and column sums in the "colsum" row. This is called as "a contigency table" and can be useful for various analyses and calculations, such as calculating proportions, percentages, or conducting statistical tests to explore the relationships between categories and groups.

Overall, a contingency table, also known as a cross-tabulation or crosstab, is a table used in statistics to summarize and display the relationship between two or more categorical variables. It provides a way to analyze and visualize how the categories of one variable are distributed across the categories of another variable. Contingency tables are particularly useful for understanding the association, dependence, or independence between categorical variables.

Now, let's use the "Titanic" dataset available in R to create a contingency table and demonstrate its use. In this example, we'll examine the relationship between passenger class (Pclass) and survival status (Survived):

### A better Example: Survival rate at Titanic

```{r}
titanic <-  read.csv(
"https://raw.githubusercontent.com/bio304-class/bio304-course-notes/master/datasets/titanic_data.csv")

# Create a contingency table for Pclass and Survived
contingency_table <- table(titanic$pclass, titanic$survived)

# Add row and column sums to the contingency table
contingency_table_with_sums <- addmargins(contingency_table)

# Print the contingency table with sums
print(contingency_table_with_sums)

```

This contingency table summarizes the counts of passengers in each combination of passenger class (1st, 2nd, 3rd) and survival status (No, Yes). It helps us understand how the survival rates vary by passenger class and whether there is a significant association between these two categorical variables.

It is good in providing a breakdown of how many passengers from each class survived and did not survive. Yet, proportions are always more interpretable in such cases. Therefore we need a contigency table instead.

```{r}
# Calculate and print the proportion table
prop_table <- prop.table(contingency_table)
# Convert proportions to percentages
percentage_table <- prop_table * 100
percentage_table <- addmargins(percentage_table)

# Print the percentage table
print(percentage_table)

# Since it is not a list, we use the `addmargins()` function to add the sums, and then calculates the proportions based on the new table with sums. 
```

::: callout-note
⚠️ The **`prop.table()`** function in R calculates the proportions of the contingency table’s entries relative to the grand total, not row-wise or column-wise. If you want the row sums (or column sums) to be 100%, you should calculate the proportions separately for each row (or column). Here’s how you can do it:

```{r}
# Calculate row proportions and convert to percentages
percentage_table_2 <- prop.table(contingency_table, 1) * 100

# Print the percentage table
print(percentage_table_2)
```
:::

For example, you can see that a higher proportion of passengers from the 1st class survived compared to the 3rd class. Such contingency tables are useful for understanding the distribution and relationships between categorical variables in your data.

You can further analyze contingency tables using statistical tests, such as the chi-squared test of independence, to determine whether the observed associations are statistically significant. Contingency tables are a valuable tool for exploring and summarizing relationships between categorical variables in a dataset.

```{r}
table(titanic$sex)
```

```{r}
barplot(table(titanic$sex))
```

```{r}
table(titanic$survived)
```

```{r}
barplot(table(titanic$survived))
```

```{r}
table_1 <- table(titanic$sex, titanic$survived)
```

```{r}
addmargins(table_1)
```

```{r}
addmargins(table_1)
prop.table(table_1)*100
prop.table(table_1, 1)*100
# you can also use proportions(table_1, margin = 1)
prop.table(table_1, 2)*100
# you can also use proportions(table_1, margin = 2)
```

```{r}
barplot(table_1, legend.text = TRUE)
```

```{r}
table_2 <- table(titanic$pclass, titanic$survived)
addmargins(table_2)
prop.table(table_2)*100
prop.table(table_2,1)*100
prop.table(table_2,2)*100
```

```{r}
barplot(table_2, legend.text = TRUE)
```

## Descriptive Statistics with Numeric Values

### Descriptive Statistics with Continues Numerical Variables

A histogram is a graphical representation of the distribution of continuous numerical data. It divides the data into intervals or bins and displays the frequency or count of data points falling into each interval as bars. Histograms are particularly useful when you want to understand the shape, central tendency, and spread of a dataset. Here's a more detailed explanation of histograms:

**Histogram Characteristics:**

-   **X-Axis:** Represents the range of values or intervals into which the data is divided.

-   **Y-Axis:** Represents the frequency or count of data points in each interval.

-   **Bars:** Vertical bars are drawn above each interval, with their heights corresponding to the frequency of data points in that interval.

-   **Bins or Intervals:** These are the subranges into which the data is divided for analysis. The number of bins and their width can be adjusted to reveal different levels of detail in the distribution.

**When to Use a Histogram:**

-   Use a histogram when you have continuous numerical data with a relatively large sample size (typically 100 values or more).

-   It helps visualize the distribution pattern, such as whether it's symmetric, skewed, bimodal, etc.

-   It provides insights into the central location (mean or median) and spread (variance or standard deviation) of the data.

-   Histograms are useful for identifying potential outliers or unusual patterns in the data.

Now, let's use a dataset available in R base to create a histogram. We'll use the "faithful" dataset, which contains information about the Old Faithful geyser eruptions:

```{r}
# Load the "datasets" package to access the "faithful" dataset 
data(faithful)
# Create a histogram of the eruption durations (in minutes) 
hist(faithful$eruptions, 
     main="Histogram of Eruption Durations", 
     xlab="Duration (Minutes)", 
     ylab="Frequency")
```

In this example, we load the "faithful" dataset and create a histogram of the eruption durations. The histogram provides a visual representation of the distribution of eruption durations, helping us understand the pattern and characteristics of this continuous numerical variable.

💡 Determining the number and width of bins (intervals) in a histogram is an important decision in the process of creating a meaningful and informative visualization. The choice of bins can significantly impact the appearance and interpretation of the histogram. Let's use the "faithful" dataset as an example to illustrate why determining bins is important:

In the context of the "faithful" dataset, which records the durations of Old Faithful geyser eruptions, here are a few scenarios to consider:

**Scenario 1: Too Few Bins (Under-Smoothing)**

```{r}
# Creating a histogram with too few bins 
hist(faithful$eruptions, 
     main="Histogram with Few Bins", 
     xlab="Duration (Minutes)", 
     ylab="Frequency", 
     breaks = 5)
```

In this scenario, we've used too few bins (only 5). The histogram appears oversimplified, and we lose details about the distribution. It doesn't provide a clear picture of the underlying patterns in the data.

**Scenario 2: Too Many Bins (Over-Smoothing)**

```{r}
# Creating a histogram with too many bins 
hist(faithful$eruptions, 
     main="Histogram with Many Bins", 
     xlab="Duration (Minutes)", 
     ylab="Frequency", 
     breaks = 50)
```

Here, we've used too many bins (50). The histogram becomes too detailed, and it may introduce noise or make it difficult to discern the overall distribution pattern. It can also exaggerate minor fluctuations in the data.

**Scenario 3: Optimal Number of Bins**

```{r}
# Creating a histogram with an appropriate number of bins (default) 
hist(faithful$eruptions, 
     main="Histogram with Default Bins", 
     xlab="Duration (Minutes)", 
     ylab="Frequency")
```

In this scenario, we've used the default number of bins (which R calculates based on a rule of thumb). The histogram strikes a balance between over-smoothing and under-smoothing. It reveals the essential characteristics of the data distribution, such as its bimodal nature.

**Why Determining Bins Is Important:**

-   **Clarity:** The choice of bins should make the distribution's key features (e.g., peaks, modes) readily apparent.

-   **Detail:** It should provide enough detail to understand the distribution's nuances without overwhelming with excessive detail.

-   **Interpretability:** The histogram should be easy to interpret and should not obscure important patterns.

-   **Accuracy:** Bins should be chosen carefully to ensure that the histogram accurately represents the underlying data distribution.

In summary, determining the appropriate number and width of bins in a histogram is essential for creating an informative visualization that effectively communicates the characteristics of the data. It requires finding a balance between oversimplification and excessive detail to make meaningful interpretations.

### Statistical measures to describe the data

1.  **Central Tendency:** Mean, Median, Mode
2.  **Dispersion/Variation**: Range, standard deviation, variance
3.  **Skewness:** Distribution plots (Histograms, bar plots)

::: callout-note
💡**Inferential Statistics** is about analyzing data (rather than summarizing it). It infers insights from the sample data for the whole population. That is, inferential statistic is to use sample data to make an inference or draw conclusion of the population. As such, inferential statistics use several statistical methods such as correlations, probability, regressions to determine how confidently we can draw conclusion about the population by relying on the sample data.
:::

```{r}
library(here)
load(here("data", "aflsmall.Rdata"))

# these two data is about Autralian football league. afl.margins contains winning margin 
# for all 176 games, afl.finalists contains names of the 400 teams that played finals
# between 1987 to 2010. 

table(afl.finalists) 
# use table functions how many times an entry appears in a vector like data

afl.margins


```

::: callout-tip
⚠️This both output doesn't say much about the data we have here! does it? In order to understand much more about, we have to compute some descriptive statistics.
:::

Let's get start with a histogram of the afl.margins data, since it should help you get a sense of what the data we're trying to describe actually look like

```{r}
hist(afl.margins) # hist function plots an histogram for you! We would get into it a bit later, but take a note of the hist() function! 
```

## Measures of Central Tendency

### Mean

It is pure and simple: average! That is, add all the values up and then divide by the total numbers of observations!

```{r}
sum(afl.margins)/176

mean(afl.margins)
```

### Median

Median of a set of value of observations is just the middle value! (MIDLLE VALUE)

```{r}
median(afl.margins) 
# since The middle values are both 30 and 31, so the median winning margin for 2010 was 30.5 points.
```

::: callout-important
⚠️ The mean is basically the "centre of gravity" of the data set: if you imagine that the histogram of the data is a solid object, then the point on which you could balance it (as if on a see-saw) is the mean. In contrast, the median is the middle observation. Half of the observations are smaller, and half of the observations are larger.
:::

![](images/meanvsmedian.png)

### Mode

It is the value that occurs most!

```{r}
table(afl.finalists) # see the frequency table
```

::: callout-note
What to use: Mean, Median, Mode?

-   If your data is nominal scale, you are more likely to use Mode! (neither mean, nor median)

-   If your data is ordinal scale, you are more likely to use median!

-   Interval, or ratio scale data (without extreme values) use mean!
:::

::: callout-note
💡**In a perfectly symmetrical distribution, the mean and the median are the same!**
:::

Overall, there is a systemic differences between mean, median when the histogram is asymmetric!

![](images/symetric.png)

## Visualization of Central Tendency

A box plot (or box-and-whisker plot) is a graphical representation that displays the distribution of a dataset and provides a summary of its central tendency, spread, and potential outliers. It consists of a box that represents the interquartile range (IQR) and "whiskers" that extend from the box to the minimum and maximum values within a defined range.

Here's a brief overview of the components of a box plot:

-   **Box:** The box represents the IQR, which contains the middle 50% of the data. The lower boundary of the box is the first quartile (Q1), and the upper boundary is the third quartile (Q3). The width of the box indicates the spread of this middle 50%.

-   **Whiskers:** The whiskers extend from the box to the minimum and maximum values within a specified range (usually 1.5 times the IQR). Data points outside this range are considered potential outliers and are plotted individually.

-   **Outliers:** Individual data points that fall outside the whiskers are marked as dots and are considered outliers.

A box plot is useful for identifying the central location, spread, and skewness of a dataset. It's also helpful for comparing the distribution of a variable across different groups, as in your case with "age" and "survived" groups.

Here's an R code example to create a box plot of age by survival status using the "titanic" dataset:

```{r}
# Create a box plot of age by survival status 
boxplot(age ~ survived, data = titanic,         
        main = "Box Plot of Age by Survival Status",         
        xlab = "Survived (0 = No, 1 = Yes)",         
        ylab = "Age")
```

This code will generate a box plot that visualizes the distribution of passenger ages for both survivors (1) and non-survivors (0). It can help you compare the age distributions of these two groups and identify any differences or patterns.

## Measures of Variability

This is all about how "spread out" are the data? How far away frm the mean/median do the observed values tend to be?

### Range

The biggest minus the smallest!

```{r}

min(afl.margins)
max(afl.margins)
range(afl.margins) # the easiest way
```

### Interquartile range

IQR is the difference between 25th and 75th quantile!

```{r}
quantile(afl.margins, probs = .1)
quantile(afl.margins, probs = .5)
quantile(afl.margins, probs = .25)
quantile(afl.margins, probs = .75)

# then it is much more easier 
quantile(afl.margins, probs = c(.25,.75))

# or
IQR(afl.margins)
```

::: callout-important
⚠️It is easier to make sense of the range! Yet, when it is come to IQR, it should be thought as the range spanned by the middle half of the data. That is, one quater of the data falls below 25th percentile/quantile, one quarter of the data above the 75th percentile/quantile, leaving the middle half of the data lying between the two!
:::

### Variance & Standard Deviation (Mean absolute deviations)

It tells how the values are spread across the data sample and it is the measure of the variation of the data points from the mean.

**Steps to Calculate Variance & Standard Deviation**

-   Find the mean, which is the arithmetic mean of the observations.

-   Find the squared differences from the mean. (The data value - mean)^2^

-   Find the average of the squared differences. (Variance = The sum of squared differences ÷ the number of observations)

-   Find the square root of variance. (Standard deviation = √Variance)

![](images/populationvssample.png)

```{r}
x <- c(2,6,5,3,2,3)
sum(x)/length(x)  # or simply mean(x)
y <- (x-(sum(x)/length(x)))
z <- (y^2)
sum(z)/(6-1) # variance 
sqrt(sum(z)/(6-1)) # Standard deviation
```

```{r}
sd(x)
var(x)
```

![](images/exap_var_sd.png)

::: callout-warning
⚠️ R always take n-1 as a default, that is it calculates var or sd for sample! So, if you like to calculate mean for population, you have to write down the equation:

(mean((x-mean(x))\^2) #variance )
:::

```{r}
var(afl.margins)
```

::: callout-note
💡Even though there is an elegance in the equation, variance has no human-friendly interpretation. It is a fundamental quantity for expressing variation, it's completely uninterpretable.
:::

```{r}
sd(afl.margins)
```

::: callout-note
💡 Interpreting standard deviations is slightly more complex. Because the standard deviation is derived from the variance, and the variance is a quantity that has little to no meaning that makes sense to us humans, the standard deviation doesn't have a simple interpretation. As a consequence, most of us just rely on a simple rule of thumb: in general, you should expect 68% of the data to fall within 1 standard deviation of the mean, 95% of the data to fall within 2 standard deviation of the mean, and 99.7% of the data to fall within 3 standard deviations of the mean.
:::

![](images/mean_.png)

### Median Absolute Deviation

MAD is a case where you use to meadian (not mean) to compute sd and var.

```{r}
mad(afl.margins) 
```

::: callout-note
Where to use (range, IQR, sd, var)

-   **Range**: Gives the full spread of data. It's vulnerable to outliers, and as a consequence it isn't often used unless you have a goo reasons to care about the extremes in the data.

-   **IQR**: Tells you where the middle half of the data sits. Its pretty roboust, and complements the median nicely. This is used a lot!

-   **MAD**: Not used very much!

-   **Variance**: Tells you the average squared deviation from the mean! It is mathematically elegant, but uninterpretable.

-   **Standard deviation**: This is square root of variance. In situations where the mean is the measure of central tendency, sd is the default! Therefore, sd is the by far the most popular and used measure of variation!
:::

## Skew and Kurtosis

These are two more descriptive statistics you sometime see reported! They are less frequent in terms of useage though than the measure of central tendency and spread we have discussed.

![](images/skew.png)

**Skewness** is basically a measure of asymmetry.

-   If data includes extreme small values, then the plot (histogram) would be negative skew (left-skew);

-   If the data includes extreme big values, then the plot (histogram) would be positive skew (right-skew)

```{r}
hist(afl.margins)
```

**Kurtosis** is a measure of the pointiness of a data set.

![](images/kurtosis.png)

## Getting the overall summary of the data

The basic idea behind the summary() function is that it prints out some useful information about whatever object. For numeric variables, we get a whole bunch of useful descriptive statistics. It gives us the minimum and maximum values (i.e., the range), the first and third quartiles (25th and 75th percentiles; i.e., the IQR), the mean and the median. In other words, it gives us a pretty good collection of descriptive statistics related to the central tendency and the spread of the data.

```{r}
summary(afl.margins)
```

::: callout-note
💡If you have more several variables in your data, describe() function from psych package is much more handy!
:::

```{r}
# get a data with several variables in it
load(here("data", "clinicaltrial.Rdata"))

# use summary function first
summary(clin.trial)
```

So, Evidently there were three drugs: a placebo, something called "anxifree" and something called "joyzepam"; and there were 6 people administered each drug. There were 9 people treated using cognitive behavioural therapy (CBT) and 9 people who received no psychological treatment. And we can see from looking at the summary of the mood.gain variable that most people did show a mood gain (mean " .88), though without knowing what the scale is here it's hard to say much more than that.

Let's see describe() function now

```{r}
library(psych) # install.packages("psych") if you don't have the package yet
psych::describe(clin.trial)

```

::: callout-warning
⚠️Notice that what the describe() function does is convert factors and logical variables to numeric vectors in order to do the calculations. These variables are marked with \* and most of the time, the descriptive statistics for those variables won't make much sense.
:::

There is even better version in which output now gives you means, standard deviations etc separately for the CBT group and the no.therapy group.

```{r}
psych::describeBy(clin.trial, group = clin.trial$therapy)
```

## Z-scores (standard scores)

Z-scores, also known as standard scores, are a way to measure how far a particular value is from the average (mean) in a group of values. They help us understand whether a value is typical or unusual when compared to the group's average and how unusual it is.

Here's a simple explanation of Z-scores and how to use them:

**Z-Score Formula:** Z-Score = (Value - Mean) / Standard Deviation

-   **Value**: The value you want to analyze.

-   **Mean**: The average of all the values in the group.

-   **Standard Deviation**: A measure of how spread out the values are from the mean.

### **Using Z-Scores:**

1.  **Understanding Typicality**: A Z-score tells you how many standard deviations a value is away from the mean. If the Z-score is 0, it means the value is exactly at the mean. Positive Z-scores indicate values above the mean, and negative Z-scores indicate values below the mean.

2.  **Empirical Rule**: Z-scores help us apply the Empirical Rule:

    -   About 68% of values are within 1 standard deviation of the mean.

    -   About 95% of values are within 2 standard deviations of the mean.

    -   About 99% of values are within 3 standard deviations of the mean.

![](images/z_score.png)

### **Examples of How to Use Z-Scores:**

1.  **Exam Scores**: Suppose the average score on a test is 75, and the standard deviation is 10. If a student scores 85, their Z-score would be (85 - 75) / 10 = 1. This means their score is 1 standard deviation above the average, which is a good performance.

2.  **Height**: For a group of people with an average height of 170 cm and a standard deviation of 10 cm, someone who is 190 cm tall would have a Z-score of (190 - 170) / 10 = 2. This indicates they are 2 standard deviations above the average, which is relatively tall.

3.  **Stock Market**: In finance, Z-scores are used to assess the risk of a stock. If a stock's Z-score is negative, it might indicate financial distress. For example, a Z-score of -2 suggests the company's financial situation is 2 standard deviations below the norm.

In essence, Z-scores provide a standardized way to compare values across different datasets. They help us identify outliers and assess how unusual or typical a data point is in relation to the mean and standard deviation of a group.

### More on Z-scores

Let's use the built-in "mtcars" dataset in R to calculate Z-scores for one of its variables and answer some statistical questions. We'll calculate Z-scores for the "mpg" (miles per gallon) variable.

```{r}
# Load the mtcars dataset
data(mtcars)

# Calculate the mean and standard deviation of mpg
mean_mpg <- mean(mtcars$mpg)
std_dev_mpg <- sd(mtcars$mpg)

# Calculate Z-scores for each observation in mpg
z_scores_mpg <- (mtcars$mpg - mean_mpg) / std_dev_mpg

# Print the first few Z-scores
head(z_scores_mpg)

```

This code calculates Z-scores for the "mpg" variable in the "mtcars" dataset. Now, let's answer some statistical questions:

**1. What is the average (mean) Z-score for mpg in the dataset?**

```{r}
mean_z_score <- mean(z_scores_mpg)
mean_z_score
```

**2. How many cars in the dataset have above-average mpg (Z-score \> 0)?**

```{r}
above_average_mpg <- sum(z_scores_mpg > 0)
above_average_mpg

```

**3. How many cars in the dataset have below-average mpg (Z-score \< 0)?**

```{r}
below_average_mpg <- sum(z_scores_mpg < 0)
below_average_mpg

```

**4. How many cars in the dataset have very high mpg (Z-score \> 2)?**

```{r}
very_high_mpg <- sum(z_scores_mpg > 2)
very_high_mpg
```

These examples demonstrate how you can calculate Z-scores for a variable in R, use them to answer statistical questions, and identify data points that fall within certain Z-score ranges.

## Correlation

Correlation is **a statistical measure that expresses the extent to which two variables are linearly related** (meaning they change together at a constant rate). It's a common tool for describing simple relationships without making a statement about cause and effect.

![](images/cor.png)

Let's get a new data to dive into it:

```{r}
# get the data
load(here("data", "parenthood.Rdata"))

# look at the data
str(parenthood)
```

```{r}
summary(parenthood)
```

```{r}
hist(parenthood$dan.sleep)
hist(parenthood$baby.sleep)
hist(parenthood$dan.grump)
```

### The strength and direction of a relationship

```{r}
plot(parenthood$dan.sleep, parenthood$dan.grump)
```

```{r}
plot(parenthood$baby.sleep, parenthood$dan.grump)
```

```{r}
cor(parenthood$dan.sleep, parenthood$dan.grump)
```

```{r}
cor(parenthood)
```

::: callout-caution
💡 So how should you interpret a correlation of, say r " .4? The honest answer is that it really depends on what you want to use the data for, and on how strong the correlations in your field tend to be. Yet, this could be a guide for interpretation:

![](images/cor2.png)
:::

```{r}
# correlation is not causation
load(here("data", "anscombesquartet.Rdata"))

cor(X1,Y1) 
cor(X2,Y2)
cor(X3,Y3)
cor(X4,Y4)
```

You'd think that these four data setswould look pretty similar to one another. They do not.

```{r}
plot(X1,Y1)
```

```{r}
plot(X2,Y2)
```

```{r}
plot(X3,Y3)
```

```{r}
plot(X4,Y4)
```

::: callout-tip
⚠️ Anscombe's quartet. All four of these data sets have a Pearson correlation of r " .816, but they are qualitatively different from one another.

Always check out scatter-plot!
:::

::: callout-caution
💡R cor() function computes Pearson Correlation by default. Perason Correlation coefficient is uselful for a lot of things, but it does have a shortcomings! It measures the strength of a linear relationship between two variables. When the relationhsip is not linear (that's why you have to look the scatter-plot first), you have to use other types of correlation such as Spearman's rank or kendall correlations.
:::

```{r}
load(here("data", "effort.Rdata"))
str(effort)
```

```{r}

plot(effort, type="b", col="red")

```

```{r}
cor(effort) # strong correlation, isn't it? 
```

::: callout-caution
⚠️there is a perfect ordinal relationship here. That is, if student 1 works more hours than student 2, then we can guarantee that student 1 will get the better grade. That's not what a correlation of r " .91 says at all.

How should we address this? Actually, it's really easy: if we're looking for ordinal relationships, all we have to do is treat the data as if it were ordinal scale! So, instead of measuring effort in terms of "hours worked", lets rank all 10 of our students in order of hours worked.
:::

![](images/cor_ex1.png)

This is basically to say that these two rankings are identical, so if we now correlate them we get a perfect relationship.

```{r}
hours.rank <- rank(effort$hours)
grade.rank <- rank(effort$grade)

# now check out the correlation
cor(hours.rank, grade.rank)
```

There is a simple version of this looking at ordinal relationship, which is spearman correlation

```{r}
cor(effort$hours, effort$grade, method = "spearman")
```

::: callout-warning
⚠️cor() function only works with numeric values! So, if your data has any factor variables, it wouldn't work out!
:::

```{r}
load(here("data", "work.Rdata"))
str(work)
```

```{r}
#try cor()  function
# cor(work)
```

In this type of situation, you have to subset the data or find other work-around! One of such work-around is correlation() function from lsr package, that would compute correlation for any data!

```{r}
# here is a work around
library(lsr) # install.packages("lsr") if you dont have the package yet! 
correlate(work)
```

## Missing Data

Real data sets very frequently turn out to have missing values. Let's start with the simplest case, in which you're trying to calculate descriptive statistics for a single variable which has missing data.

```{r}
partial <- c(10,20,NA,30)
```

```{r}
mean(partial)
```

To fix this, all of the descriptive statistics functions that I've discussed in this chapter (with the exception of cor() which is a special case I'll discuss below) have an optional argument called na.rm, which is shorthand for "remove NA values". By default, na.rm = FALSE, so R does nothing about the missing data problem. Let's try setting na.rm = TRUE

```{r}
mean(partial, na.rm = TRUE)
```

::: callout-note
⚠️Remember, cor() function is an exeption. You have to decide what to do with NA, when you are using cor() function
:::

```{r}
#lets get a data with some missing values
load(here("data", "parenthood2.Rdata"))
str(parenthood2)
```

```{r}
# let's try to do cor()
cor(parenthood2)
```

To make R behave more sensibly in this situation, you need to specify the use argument to the cor() function. There are several different values that you can specify for this, but the two that we care most about in practice tend to be "complete.obs" and "pairwise.complete.obs". If we specify use = "complete.obs", R will completely ignore all cases (i.e., all rows in our parenthood2 data frame) that have any missing values at all. So, for instance, if you look back at the extract earlier when I used the head() function, notice that observation 1 (i.e., day 1) of the parenthood2 data set is missing the value for baby.sleep, but is otherwise complete? Well, if you choose use = "complete.obs" R will ignore that row completely: that is, even when it's trying to calculate the correlation between dan.sleep and dan.grump, observation 1 will be ignored, because the value of baby.sleep is missing for that observation. Here's what we get:

```{r}
cor(parenthood2, use = "complete.obs")
```

The other possibility that we care about, and the one that tends to get used more often in practice, is to set use = "pairwise.complete.obs". When we do that, R only looks at the variables that it's trying to correlate when determining what to drop. So, for instance, since the only missing value for observation 1 of parenthood2 is for baby.sleep R will only drop observation 1 when baby.sleep is one of the variables involved: and so R keeps observation 1 when trying to correlate dan.sleep and dan.grump. When we do it this way, here's what we get:

```{r}
cor(parenthood2, use = "pairwise.complete.obs")
```

## Summary

Calculating some basic descriptive statistics is one of the very first things you do when analysing real data, and descriptive statistics are much simpler to understand than inferential statistics. In this lecture, we talked about the following topics:

-   **Measures of central tendency.** Broadly speaking, central tendency measures tell you where the data are. There's three measures that are typically reported in the literature: the mean, median and mode.

-   **Measures of variability.** In contrast, measures of variability tell you about how "spread out" the data are. The key measures are: range, standard deviation, interquartile range.

-   **Getting summaries** of variables in R. Since this book focuses on doing data analysis in R, we spent a bit of time talking about how descriptive statistics are computed in R.

-   **Standard scores.** The z-score is a slightly unusual beast. It's not quite a descriptive statistic, and not quite an inference. Make sure you understand the basic! it'll come up again later.

-   **Correlations.** Want to know how strong the relationship is between two variables? Calculate a correlation.

-   **Missing data.** Dealing with missing data is one of those frustrating things that data analysts really wish the didn't have to think about. In real life it can be hard to do well.
