---
title: "simple_linear_regression_exercise"
author: "Hakan Mehmetcik"
format: pdf
editor: visual
execute: 
  echo: true
  warning: true
  output: asis
df-print: kable
---

## Inferential Statistics

After having a summary of the data via descriptive statistics, we would answer questions using statistical inference (inferential statistics). In fact, descriptive statistics is one of the smallest parts of statistics, and one of the least powerful. The bigger and more useful part of statistics is inferential statistics that lets you make inferences about data.

::: callout-note
ðŸ’¡ **Inferential statistics** involves making conclusions about a larger group (or population) based on a smaller group (or sample) drawn from it. This is incredibly useful because it's often impractical or impossible to collect data from every single member of a population.

Inferential statistics is also traditionally divided into two main areas: **estimation and hypothesis testing**.

-   **Estimation:** This involves using sample data to estimate the value of an unknown parameter of the population. For instance, if you wanted to know the average height of all adults in a city, you might measure the heights of a sample group from the city. An estimate from this sample, like the average height of these sampled individuals, helps predict the average height of all adults in the city. Estimation can be of two types:

    -   **Point estimation** provides a single best guess for a parameter (like the average height).

    -   **Interval estimation** gives a range within which the parameter likely falls (for instance, the average height is between 5 feet 7 inches and 5 feet 9 inches, with a certain level of confidence).

-   **Hypothesis Testing:** This is about making decisions or testing claims about a population based on sample data. For example, suppose a fast-food franchise claims that their service time is less than 5 minutes on average. You might test this claim by sampling some service times at various outlets. Hypothesis testing involves: Setting up two opposing statements (hypotheses): one that represents the claim (e.g., average service time is less than 5 minutes) and one that represents the alternative (e.g., average service time is 5 minutes or more). Analyzing sample data to see which statement is more likely to be true, given the data.

    Both estimation and hypothesis testing are tools that help us draw conclusions about a population based on sample observations. They use the principles of probability to account for the fact that sampling naturally involves some amount of random variation. This way, even if the sample isn't exactly representative of the whole population, we can still make reasonably accurate and confident predictions about the population.
:::

## Random Variables and Probability Distributions

When we speak of numeric variables, we are actually speaking of a **random variable**, which is a numeric summary of a random phenomenon.

**A discrete random variable** is one that has countable numbers of outcomes, while **continiues random variable** is a random varibale with an uncountable numbers of outcomes.

Examples of discrete random variables includeÂ **the number of children in a family**, the Friday night attendance at a cinema, the number of patients in a doctor's surgery, the number of defective light bulbs in a box of ten.

TheÂ **mass, temperature, energy, speed, length**, and so on are all examples of continuous variables.

**A probability Distribution** is a summary of a random variable that gives all possible values of the random variable along with their probability of occurring.

![](images/Discrete-and-continuous-random-variables.png)

## R has a function for

![](images/Screenshot%202022-12-06%20at%2022.23.51.png)

## The Normal Distribution (Gaussian Distribution)

Normal distribution is one of the most important distribution in statistics. Normal distribution is known as "the bell curve" or a "Gaussian distribution" too.

The normal distribution is symmetric, uni modal, and it is a distribution for numeric continuous variable.

In normal distribution there are two parameters:

-   the parameter mean (defines the center)

-   the variance (or sd) (defines the spread)

The area **under the curve** tells you the probability that an observation falls within a particular range. The solid lines plot normal distributions with mean Î¼ " 0 and standard deviation Ïƒ " 1. The shaded areas illustrate "areas under the curve" for two important cases. In panel a, we can see that there is a 68.3% chance that an observation will fall within one standard deviation of the mean. In panel b, we see that there is a 95.4% chance that an observation will fall within two standard deviations of the mean.

![](images/Screenshot%202022-12-06%20at%2022.41.15.png)

![](images/Screenshot%202022-12-06%20at%2022.41.44.png)

## Sampling Distributions

In almost every situation of interest, what we have available to us as researchers is a sample of data. We might have run experiment with some number of participants; a polling company might have phoned some number of people to ask questions about voting intentions; etc. Regardless: the data set available to us is finite, and incomplete.

Statistics are numbers that are calculated from sample and that generally speaking estimate parameters. For exm, sample mean x(\^) estimates the population mean u, sample proportion p(\^) estimates the population proportion.

What kinds of things would we like to learn about? And how do we learn them? These are the questions that lie at the heart of inferential statistics, and they are traditionally divided into two "big ideas": estimation and hypothesis testing.

#### An Example for Sampling

```{r}
# Lets make fake IQ scores
set.seed(178)
IQ <- rnorm(n=1000, mean=100, sd=15) # generate IQ scores
IQ <- round(IQ) # IQs are whole numbers

summary(IQ)
```

```{r}
# let's now have 5 samples out of this
IQ.five1 <- round(rnorm(5, mean = 100, sd=15))
IQ.five2 <- round(rnorm(5, mean = 100, sd=15))
IQ.five3 <- round(rnorm(5, mean = 100, sd=15))
IQ.five4 <- round(rnorm(5, mean = 100, sd=15))
IQ.five5 <- round(rnorm(5, mean = 100, sd=15))

replication <- as.data.frame(rbind(IQ.five1, IQ.five2, IQ.five3, IQ.five4, IQ.five5))
replication$mean <-  apply(replication[,-1], 1, mean) # add rowwise mean 
replication
```

## **The Central limit theorem (CLT)**

The Central Limit Theorem (CLT) is a fundamental concept in statistics that helps explain why many distributions in the real world tend to resemble a bell-shaped curve known as the normal distribution, especially as more data points are considered.

Let's take and example:

Imagine you're at a large family reunion and you want to know the average age of all attendees. Instead of asking everyone their age (which might be time-consuming and impractical), you decide to randomly select groups of, say, 30 family members and calculate the average age of each group.

You do this multiple times, creating several groups of 30 and calculating the average age for each group. Now, if you were to create a chart (a histogram) of all these averages, the CLT tells us that this chart will likely resemble a bell-shaped curve (the normal distribution), regardless of the actual age distribution of all your family members.

The key points about the Central Limit Theorem are:

It applies when you are looking at averages or sums of a sample. In our example, we were looking at the average age of groups. The larger the sample size used in each group, the more the distribution of these averages will resemble a normal distribution. Even if the original data (ages of all attendees) are not normally distributed, the distribution of the averages will tend to be normal as the sample size increases. This tendency towards a normal distribution happens regardless of the shape of the original data distribution. Whether the actual ages are skewed to the left or right, or even if they follow no apparent pattern at all, the average ages calculated over many samples will form a normal distribution. Why is this useful? The CLT is powerful because it allows statisticians to make inferences and conduct hypothesis tests using the normal distribution. This is beneficial because the properties of the normal distribution are well-understood and can be applied to many real-world scenarios, making it easier to predict probabilities and make decisions based on sample data.

**How about another example:**

```{r}
hist(IQ.five1)
hist(IQ)
```

::: callout-note
ðŸ‘‹ no matter what shape your population distribution is, as N increases the sampling distribution of the mean starts to look more like a normal distribution!
:::

Thus, Central Limit theorem basically suggest that:

â€¢ The mean of the sampling distribution is the same as the mean of the population

â€¢ The standard deviation of the sampling distribution (i.e., the standard error) gets smaller as the

sample size increases

â€¢ The shape of the sampling distribution becomes normal as the sample size increases.

![](images/Screenshot%202022-12-13%20at%2020.10.53.png)

![](images/Screenshot%202022-12-13%20at%2020.10.53.png)

## Estimating Population Parameters

It is all about the procedure for estimating population parameters from a sample of data. How to do that?

![](images/1_6.jpg)

### The mean of the sampling distribution is the same as the mean of the population

First guess is about the estimation of population mean. This exactly equals to sample mean. As simple as it is!

![](images/Screenshot%202022-12-13%20at%2020.17.01.png)

```{r}
IQ.2 <- round(rnorm(10000, mean = 100, sd=15))
sample1 <- sample(IQ.2,10)
mean1 <- mean(sample1)
sample2 <- sample(IQ.2,10)
mean2 <- mean(sample2)
sample3 <- sample(IQ.2,10)
mean3 <- mean(sample3)
sample4 <- sample(IQ.2,10)
mean4 <- mean(sample4)
sample5 <- sample(IQ.2,10)
mean5 <- mean(sample5)
sample6 <- sample(IQ.2,10)
mean6 <- mean(sample6)
sample7 <- sample(IQ.2,10)
mean7 <- mean(sample7)
sample8 <- sample(IQ.2,10)
mean8 <- mean(sample8)
sample9 <- sample(IQ.2,10)
mean9 <- mean(sample9)
sample10 <- sample(IQ.2,10)
mean10 <- mean(sample10)
sample11 <- sample(IQ.2,10)
mean11 <- mean(sample11)
sample12 <- sample(IQ.2,10)
mean12 <- mean(sample12)
sample13 <- sample(IQ.2,10)
mean13 <- mean(sample13)
sample14 <- sample(IQ.2,10)
mean14 <- mean(sample14)
sample15 <- sample(IQ.2,10)
mean15 <- mean(sample15)
sample16 <- sample(IQ.2,10)
mean16 <- mean(sample16)
sample17 <- sample(IQ.2,10)
mean17 <- mean(sample17)
sample18 <- sample(IQ.2,10)
mean18 <- mean(sample18)
sample19 <- sample(IQ.2,10)
mean19 <- mean(sample19)
sample20 <- sample(IQ.2,10)
mean20 <- mean(sample20)
sample21 <- sample(IQ.2,10)
mean21 <- mean(sample21)
sample22 <- sample(IQ.2,10)
mean22 <- mean(sample22)
sample23 <- sample(IQ.2,10)
mean23 <- mean(sample23)
sample24 <- sample(IQ.2,10)
mean24 <- mean(sample24)
sample25 <- sample(IQ.2,10)
mean25 <- mean(sample25)
sample26 <- sample(IQ.2,10)
mean26 <- mean(sample26)
sample27 <- sample(IQ.2,10)
mean27 <- mean(sample27)
sample28 <- sample(IQ.2,10)
mean28 <- mean(sample28)
sample29 <- sample(IQ.2,10)
mean29 <- mean(sample29)
sample30 <- sample(IQ.2,10)
mean30 <- mean(sample30)

(mean1+mean2+mean3+mean4+mean5+mean6+mean7+mean8+mean9+mean10+mean11+mean12+mean13+mean14+mean15+mean16+mean17+mean18+mean19+mean20+mean21+mean22+mean23+mean24+mean25+mean26+mean27+mean28+mean29+mean30)/30
```

![](images/Screenshot%202022-12-13%20at%2020.43.36.png)

::: callout-note
âš ï¸ Central Limit Theorem only works:

1.  Random Sampling
2.  Normal Distribution
    1.  if not, take at least 30 sample!: If the population is not normally distributed, the sample size should be at least 30, according to the Central Limit Theorem, in order to have a sampling distribution that is approximately normal.
3.  Independence condition
    1.  sampling with replacement , then we are ok!

    2.  sampling without replacement (the 10% **rule!):** If you are sampling without replacement, you typically can't use the Central Limit Theorem unless your sample size is less than or equal to 10% of the population. This is known as the 10% rule.

### Sampling Procedure

**Simple Random sample**: Selecting black or white chips from a box without replacement

![](images/Screenshot%202022-12-06%20at%2023.04.00.png)

![![](images/Screenshot%202022-12-06%20at%2023.06.46.png)](images/Screenshot%202022-12-06%20at%2023.04.00.png)

![](images/Screenshot%202022-12-06%20at%2023.06.46.png)

âš ï¸ Most samples are not simple random sample! More generally though, it's important to remember that random sampling is a means to an end, not the end in itself.
:::

## Point estimation

The thing that has been missing from this discussion is an attempt to quantify the amount of uncertainty that attaches to our estimate. It's not enough to be able guess that, say, the mean IQ of undergraduate statistic students is 115 (yes, I just made that number up). We also want to be able to say something that expresses the degree of certainty that we have in our guess. For example, it would be nice to be able to say the chance of having a 89 IQ score for a random pick among these students!

```{r}
# Let's recall IQ score data we have cretaed

# Point Estimation
mean_est <- mean(IQ) # Mean IQ score
sd_est <- sd(IQ) # Standard deviation of IQ scores

# Probability calculation for if a student has 89 IQ score among this sample
prob <- pnorm(89, mean=mean_est, sd=sd_est)

# Print result
print(paste("The probability of a random student having an IQ of 89 or less is:", prob))


```

-   The **`pnorm`** function is used to calculate the probability of a random student having an IQ of 89. This function gives the cumulative distribution function for the normal distribution.

-   You can also do this with calculating z score

```{r}
# it is much easier if you use pnorm function, yet z-score functions works too! 
z_score <- function(x,y,s) {
 (x-y)/s
} 
z_score_est <- z_score(89,mean_est,sd_est)
prob <- pnorm(z_score_est)

# Print result
print(paste("The probability of a random student having an IQ of 89 or less is:", prob))
```

So, what is the difference?

In statistics, the `pnorm` function in R is used to calculate the cumulative distribution function (CDF) for a normal distribution. If you know the true population mean (Î¼) and standard deviation (Ïƒ), you can use the `pnorm` function directly to calculate probabilities.

Here's an example:

```{r}
# True population parameters
mu <- 100
sigma <- 15

# Calculate the probability that a random variable X is less than 110
prob <- pnorm(110, mean=mu, sd=sigma)

# Print the result
print(paste("The probability that X < 110 is:", prob))
```

However, if you don't know the true population parameters and you're working with sample data, you would typically use the z-score and critical z-values for your calculations. The z-score standardizes your data to a standard normal distribution (mean = 0, sd = 1), which allows you to calculate probabilities using the standard normal table.

Here's an example:

```{r}
# Sample data
data <- rnorm(n=30, mean=100, sd=15)

# Calculate the sample mean and standard deviation
x_bar <- mean(data)
s <- sd(data)

# Calculate the z-score for a value of 110
z <- (110 - x_bar) / (s / sqrt(length(data)))

# Calculate the probability that the sample mean is less than 110
prob <- pnorm(z)

# Print the result
print(paste("The probability that the sample mean < 110 is:", prob))
```

In this code, `pnorm(z)` gives the probability that a standard normal random variable is less than `z`.

How about looking at greater than values! Simple, isn't it?

```{r}

# Calculate the probability that a standard normal random variable is greater than z
prob <- 1 - pnorm(z)

# Print the result
print(paste("The probability that a standard normal random variable is greater than", z, "is:", prob))

```

## Estimating a confidence Interval

Another example, it would be nice to be able to say the probability that a randomly selected student's IQ will between two values.

```{r}

# Interval Estimation
alpha <- 0.05 # significance level for a 95% confidence interval
error <- qnorm(1 - alpha/2) * sd_est/sqrt(length(IQ)) # margin of error

lower_bound <- mean_est - error # lower bound of the confidence interval
upper_bound <- mean_est + error # upper bound of the confidence interval


# Print results
print(paste("95% Confidence Interval for Mean IQ: (", lower_bound, ",", upper_bound, ")"))

```

ok, the probability that a randomly selected student's IQ will be between 89 to 101?

```{r}
# having 89
having_89 <- pnorm(89, mean_est, sd_est)
having_101 <- pnorm(101, mean_est, sd_est)

# print 
cat("The chance for having a random pick between 89 and 101 is:",  having_101 - having_89)
```

# Functions and Models

## Functions

A function: You take some kind of input and you get some kind of output!

The hint here is that you can get ONE specific output from any specific input.

Ex: **A national park contains foxes that prey on rabbits. Â The table below gives the two populations, F and R, over an 11-month period, where t=0 means January, t=1 means February, and so on.**

Sure, here is the table in R markdown format:

| Month | Rabbits | Foxes |
|-------|---------|-------|
| 0     | 1,000   | 150   |
| 1     | 750     | 143   |
| 2     | 567     | 125   |
| 3     | 500     | 100   |
| 4     | 567     | 75    |
| 5     | 750     | 57    |
| 6     | 1,000   | 50    |
| 7     | 1,250   | 57    |
| 8     | 1,433   | 75    |
| 9     | 1,500   | 100   |
| 10    | 1,433   | 125   |

This table shows the number of rabbits and foxes over a period of 10 months. The 'Rabbits' and 'Foxes' columns represent the population of each species at the end of each month.

1.  Is F a function of t?

Yes, because for each value of t, there is axactly one value of F.

2.  Is R a function of F?Â 

No, beacuse when F=57 R=750 or R=1250.

ðŸ’¡ A function is generally denoted by f(x) where x is the input. For example, in the function f(x) = x\^2, the function f(x) takes the value of "x" and then squares it!

::: callout-note
ðŸ‘‹ In statistics, **a distribution is a function that shows the possible values for a variable and how often they occur**. It's a description of the relative numbers of times each possible outcome will occur in a number of trials. Distributions can be represented in various ways, such as with graphs or probability tables.
:::

# Simple Linear Regression

## What is regression?

Statistical models to explore the relationship a response (dependent) and explanatory (independent variable)

With regression, you can predict the values of the response variable with the use of given values of explanatory variable.

ðŸ‘‹ Make sure that you know the followings!

**linear regression :** when the response variable is numeric

**logistic regression:** when the response variable is logical (True -false)

**simple linear/logistic regression:** when there is one explanatory variable

**multiple linear/logistic regression:** when there is more than one explanatory variable

::: callout-note
ðŸ’¡ Here is a a list of possible linear relations:

![](images/clipboard-832738691.png)

ðŸ‘‹ **lm() function to simulate linear models in R:**

In R, linear models are typically fitted using the `lm()` function. This function fits a linear model to data by finding the line of best fit that minimizes the total error of the model.

Here's an example of how you might use it:

```{r}
# Create some data
x <- c(1, 2, 3, 4, 5)
y <- c(2, 3, 5, 7, 11)

# Fit a linear model
model <- lm(y ~ x)

# Print the model summary
summary(model)
```

In this code, `lm(y ~ x)` fits a linear model to the data with `y` as the response variable and `x` as the predictor variable. The `summary(model)` function then prints a summary of the model, including the coefficients, residuals, and other diagnostic measures.

âš ï¸ The **`summary()`** function provides a lot of information. Here's what some of it means:

-   **Call**: This shows the function call that you used to fit the model. (`lm(y ~ x) in the example above)`

-   **Residuals**: This section provides summary statistics for the residuals, which are a measure of how far off our model's predictions are for each point.

-   **Coefficients**: This is one of the most important parts of the output. It gives the coefficients of the model, the standard error of these coefficients, and the t-value and p-value of the hypothesis test that the coefficient is different from 0. The "Estimate" column gives the coefficients of the intercept and the predictor variable(s). In this case, the coefficient for **`x`** is the slope of the line of best fit.

-   **Residual standard error**: This is the standard deviation of the residuals. It gives a measure of how wrong our model's predictions are likely to be.

-   **Multiple R-squared**: This is the proportion of variance in the response variable that can be explained by the predictor variables. It provides a measure of how well the model fits the data.

-   **F-statistic and p-value**: The F-statistic is a measure of how much better the model fits the data than a model that has no predictor variables. The p-value associated with this F-statistic is the probability of observing such an F-statistic, or one more extreme, under the null hypothesis that the model with no predictor variables fits the data as well as our model.
:::

## Exercise 1:

For this exercise, we have bivariate data on a group of college students: the total amount (in dollars) spent on textbooks throughout their college career, and their GPA. The following linear regression model was used to predict GPA from number of dollars (in hundreds) spent:

**Predicted GPA = 2.84 + .04\*Dollars**

### Question 1:

What is the predicted GPA of a student who spent a total of \$970 on textbooks in college?

```{r}
predictedgpa <- function(x) {
  2.84 + 0.04*x/100
}

predictedgpa(970)
```

### Question 2:

If a student spent \$0 on textbooks in college and graduated with a GPA of 3.71, what is her residual?Â 

```{r}
# residual = true-value - predicted_value 

3.71- predictedgpa(0)
```

### Question 3:

If a student spent \$1,450 on textbooks and graduated with a GPA of 2.91, what is his residual?Â *(Please indicate whether the residual is positive or negative in your response, andÂ **round to 2 decimal places**.)*

```{r}
2.91 - predictedgpa(1450)
```

### Question 4:

Calculate how much money a freshman has to spend to get 4 GPA

```{r}
# 4 = 2.84 + 0.04*x/100
((4-2.84)/0.04)*100
```

A freshman learned of this study and calculated that she would need to spend \$2,900 on textbooks to earn a 4.0 GPA. (You have just confirmed this calculation using the equation above). She decided to buy all of her textbooks new (rather than second-hand and cheaper) to help boost her GPA. Is she using the model in a statistically-sound way?

**Answer** : Of course not. No!

## Exercise 2:

For this exercise, we have Swedish Motor Insurance dataset, which was compiled by the Swedish Committee on the Analysis of Risk Premium in Motor Insurance. The data are cross-sectional, describing third party automobile insurance claims for the year 1977. The outcomes of interest are the number of claims (the frequency) and sum of payments (the severity), in Swedish kroners. Outcomes are based on 5 categories of distance driven by a vehicle, broken down by 7 geographic zones, 7 categories of recent driver claims experience and 9 types of automobile.

```{r}
# required libraries 
library(here)
library(fst)
library(tidyverse) 

# get the data 
motor_insurance <-read.csv(here("data", "SwedishMotorInsurance.csv"))
```

Let's look at the data first!

```{r}
str(motor_insurance)
# View(motor_insurance)
```

### Question 1:

Calculate adequate descriptive statistic for payment and claims in this data

**Means**:

```{r}
mean(motor_insurance$Claims)
mean(motor_insurance$Payment)
```

**correlations**

Check if there is a correlation with these two variables and plot it

```{r}
cor(motor_insurance$Claims, motor_insurance$Payment)

```

**plot**

```{r}
plot( motor_insurance$Claims, motor_insurance$Payment)

```

Let's now put a straight lines to our ploting of claims vs payments

```{r}
reg <- lm(Payment~Claims, data = motor_insurance)
plot( motor_insurance$Claims, motor_insurance$Payment)
abline(reg = reg, col="red")
```

```{r}
summary(reg)
```

Here, the straight line is defined by two things:

**intercept** : The y value at given x is zero.

**slope**: The amount of change in y when a unit of change happens in x

So, the following is our model (remember the funcitons and models)

**equation**: y= intercept + slope \* x

Thus, we can say that

payment = -3362 + 5020\*Claims

That is, every new claim results in 5020 SEK payment with an initial negative of 3362, which basically says if no claims there would be gain of 3362 for insurance companies. This model could be meaningful depending on the context. If the negative intercept makes sense in your context (i.e., the insurance company has other sources of income or factors that could lead to a gain even when there are no claims), then the model could be valid. However, if the negative intercept doesnâ€™t make sense (i.e., itâ€™s not possible for the insurance company to have a gain when there are no claims), you might want to reconsider the model.

## Exercise 3:

We have compiled the world record times for track events like the 100m dash and record distances for field events like the shotput into a single dataset.Â  This dataset includes information on the person who broke the record, his/her nationality, where the record was broken, and the year it was broken.Â  Note that not all world records are broken during the Olympics, with many occurring in regional or national competitions.

```{r}
wr <- read.csv(here("data", "worldrecord.csv"))
```

Let's start with eximing the data a bit

### Question 1:

How many different types of events (e.g. "Mens 100m," "Womens shotput," etc.) are represented in the dataset?

```{r}
# use table function for how many question
table(wr$Event)
```

```{r}
# or you can use count function in plyr package for better list
plyr::count(unique(wr$Event)) # instead of calling the packeag library(plyr) we used plyr::
```

### Question 2:

In what year did Usain Bolt first break the world record for the men's 100m dash?

```{r}
wr[wr$Athlete== "Usain Bolt",]
```

### Question 3:

Who was the first woman to break the women's 1 mile world record with a time of less than 260 seconds?

```{r}
wr[wr$Event=="Womens Mile",]
```

### Question 4:

Which variable tells us the record setting time or distance? What type of variable is this?

```{r}
str(wr)
```

### Question 5:

Which variable tells us when the record was broken? What type of variable is this?

```{r}
str(wr)
```

### Question 6:

For each sex, we will begin our analysis by generating aÂ **scatterplot**Â of shotput distance and year. Why?

**Answer**: The scatterplot will show us if these two numeric variables are linearly related.

```{r}

# create a subset of mens shotput event
mensshotput <- wr[wr$Event=="Mens Shotput",]

# plot distance and year 
plot(mensshotput$Record, mensshotput$Year)

# create a subset of womens shotput event
womensshotput <- wr[wr$Event=="Womens Shotput",]
# plot distance and year 
plot(womensshotput$Record, womensshotput$Year)

```

### Question 7:

What will we be able to determine once we fit aÂ **linear model**Â to this shotput world record data?Â and What is the equation for the linear model that predicts the World Record shotput distance forÂ men?

```{r}
lm_mdl_men <- lm(Record~Year, data = mensshotput)
summary(lm_mdl_men)

# since year doesn't start with 0, recode 
mensshotput$Year <- mensshotput$Year - min(mensshotput$Year)
lm_mdl_men <- lm(Record~Year, data = mensshotput)
summary(lm_mdl_men)

# In the model, We will be able to report the average rate of change in world record shotput distance per year.
# the model now simply says that with an interceot of 17.90 every additional year effect the record 0.13 meter. 
```

### Question 9:

What is theÂ dependentÂ variable in our linear models?

**Answer**: Shotput distance

### Question 10:

How many records are in theÂ menshotÂ data frame?

```{r}
str(mensshotput)
```

### Question 11:

How many records are in theÂ womenshotÂ data frame?

```{r}
str(womensshotput)
```

### Question 12:

Is a linear model appropriate for theÂ men's shotput data?

```{r}
# plot distance and year 
plot(mensshotput$Record, mensshotput$Year)
# plot distance and year 
plot(womensshotput$Record, womensshotput$Year)
```

### Question 13:

What can we sayÂ about the modelsÂ for men and women?

```{r}
lm_mdl_women <- lm(Record~Year, data = womensshotput)
summary(lm_mdl_women)

# since year doesn't start with 0, recode 
womensshotput$Year <- womensshotput$Year - min(womensshotput$Year)
lm_mdl_women <- lm(Record~Year, data = womensshotput)
summary(lm_mdl_women)
```

It can be argued that given the slope of 0.23, The rate of change is greater for women than for men.

### Draw a conclusion

Based on scatterplots of the men's and women's world record shotput distance, both of these events follow a strong, **positive** linear relationship over time. The men's world record distance increases by an average of 0.13 meters per year, while the women's record distance increases by an average of 0.23 meters per year. Because the intercept estimate is the value of the record distance when **year** is equal to 0, it is not interpretable in the context of the problem. To correct this, we have **transpoze** the year variable by using the minimum year as 0. Both transpozed linear models fit the data well, with R-squared values for the men's and women's models equal to 17.9 and 14.83, respectively. Overall, the rate of change record per unit increase in year is greater for women than for men.

## Exercise 4

Keep on working with World Record Data set. Now, our **Reserach Questions** is "How have world record times for the men's and women's mile event changed over the years?"

Yet, let's start with some basic of linear regression!

### Question 1:

When fitting a model to data, what should you doÂ **first**Â to examine the data?

**Answer**: Create a scatterplot of the two variables of interest.Â 

### Question 2:

When fitting a linear model, what will tell you theÂ **proportion of variance**Â in the dependent variable that can be explained by the independent variable?

**Answer**: the R-squared value

### Question 3:

Which scatterplot shows aÂ **stronger**Â linear relationship between World Record times in the Mile and Year:

```{r}
# create a subset of mens mile event
mensmile <- wr[wr$Event=="Mens Mile",]

# plot mile and year 
plot(mensmile$Record, mensmile$Year)

# create a subset of womens mile event
womensmile <- wr[wr$Event=="Womens Mile",]
# plot mile and year 
plot(womensmile$Record, womensmile$Year)
```

### Question 4:

On average, how manyÂ *seconds*Â do men trim off the world record time in the Mile each year?Â 

```{r}
men_mdl_mile <- lm(Record~Year, data = mensmile)
summary(men_mdl_mile)
```

### Question 5:

On average, how manyÂ *seconds*Â do women trim off the world record time in the Mile each year?

```{r}
women_mdl_mile <- lm(Record~Year, data = womensmile)
summary(women_mdl_mile)
```

### Question 6:

How manyÂ **years**Â would you predict it would take for the men's mile record to decrease by one full second? Use the model equation to help you answer the question.

```{r}
# model equation record = 1007.47 + (-0.393)*Year
1/0.393
```

### Question 7:

How manyÂ **years**Â would you predict it would take for the women's mile record to decrease by one full second? Use the model equation to help you answer the question.

```{r}
# model equation record = 2189.2 + (-0.972)*Year
1/0.972
```

### Question 8:

What proportion of variance in the men's and women's World Record times in the Mile can be explained by year?Â 

```         
Adjusted R-squared:  0.9767 # for men
Adjusted R-squared:  0.8861 # for women 
```

### Question 9:

Is the following is a reasonable conclusion to draw from this analysis?

World record times in the Mile have decreased linearly over the last several decades for both men and women.

**Answer**: YES! We can claim that World record times in the Mile have decreased linearly over the last several decades for both men and women.

### **Draw a conclusion**

Based on scatterplots of the men's and women's world record mile event, both of these events follow a strong, **negative** relationship over time. For both groups, the assumption of linearity appears to be satisï¬ed. The men's world record mile time decreases by an average of 0.393 seconds per year, while the women's record distance decreases by an average of 0.976 seconds per year. Because the intercept estimate is the value of the record time when year is equal to 0, it is not interpretable in the context of the problem. Both linear models ï¬t the data well, with R-squared values for the men's and women's models equal to 0.976 and 0.886 , respectively. For the men's world record, 97.7% of the correct is explained by the linear model of year, while for the female world record, 88.6% of the correct in performance can be explained by the linear model of year.

## Exercise 4

Keep on working with World Record Data set. Now, **We want to find the best-fitting linear model for men's pole vault world records since 1970.**

To do that, first, let's create a new data frame that contains the world record cases in the men's pole vault event in years 1970 and later.Â 

```{r}
menspole <- wr[wr$Event=="Mens Polevault" & wr$Year>= 1970,]
```

### Question 1:

What is the standing world record height (in meters) for men's pole vault?

```{r}
max(menspole$Record)
```

### Question 2:

In what year did the pole vault record first exceedÂ **6 meters**?

```{r}
menspole[menspole$Record>6,]
```

### Question 3:

Let's now Create a scatterplot showing the men's pole vault records since 1970 as a function of year. Fit a linear model to the data.

```{r}
men_mdl_pole <- lm(Record ~Year, data = menspole)
plot(menspole$Year, menspole$Record)
abline(reg = men_mdl_pole, col="red")
```

### Question 4:

Is the following best describes how the record has changed over time?

**The record pole vault height steadily increases over time.**

**Answer**: Yes, indeed, the record pole vault height steadily increases over time.

### Question 5:

Report the coefficient estimates for the linear model that describes the change in the men's pole vault world record since 1970

```{r}
summary(men_mdl_pole)
```

What is the intercept and slope?

intercept = 51.854

Slope = 0.0291

### Question 6:

What best describes how the men's pole vault world record has changed since 1970?

**Answer**: The record has increased by an average of 0.03 meters per year since 1970.

# Categorical Explanatory Variables

Categorical variables require special attention in regression analysis because, unlike dichotomous or continuous variables, they cannot by entered into the regression equation just as they are.Â Â Instead, they need to be recoded into a series of variables which can then be entered into the regression model.Â  There are a variety of coding systems that can be used when recoding categorical variables.Â  Regardless of the coding system you choose, the overall effect of the categorical variable will remain the same. Ideally, you would choose a coding system that reflects the comparisons that you want to make.Â  For example, you may want to compare each level of the categorical variable to the lowest level (or any given level).Â  In that case you would use a system calledÂ **simple coding**.Â  Or you may want to compare each level to the next higher level, in which case you would want to useÂ **repeated coding**.Â  By deliberately choosing a coding system, you can obtain comparisons that are most meaningful for testing your hypotheses. Below is a table listing various types of contrasts and the comparison that they make. **We should note that some forms of coding make more sense with ordinal categorical variables than with nominal categorical variables!**

## Exercise 1:

Let's get some data to work on categorical variable and regression! We have here a fish dataset, which records of 7 common different fish species in fish market sales. With this dataset, a predictive model can be performed using machine friendly data and estimate the weight of fish can be predicted.

```{r}
fish <- read.csv(here("data", "Fish.csv"))
str(fish)

```

For convenience, we have subset data for only four species!

```{r}
species <-c("Bream", "Perch", "Pike", "Roach")
fish <- fish[fish$Species==species,]
```

### Question 1:

Create a distribution plot for this dataset!

```{r}
barplot(table(fish$Species))
```

::: callout-tip
ðŸ‘‹ A better way to visualize this could be using ggplot faceting options

```{r}
ggplot(data = fish, aes(x=Weight)) + 
  geom_histogram( bins = 9) +
  facet_wrap(~Species)
```
:::

### Question 2:

What are the basic descriptive metrics (mean)

```{r}
# in order to calculate means for each group we use aggregate() in base R
aggregate(fish$Weight, list(fish$Species), FUN=mean)
```

::: callout-tip
::: callout-tip
ðŸ‘‹ A better way to visualize this could be using group_by() in dplyr pacakge

```{r}
fish |> 
  group_by(Species) |> 
  summarise(mean_wight = mean(Weight))
```
:::
:::

### Question 3:

Report the coefficient estimates for the linear model that describes the change in the weight for different fish species

```{r}
lm(formula = Weight~Species, data = fish)
```

### Question 4:

What is the problem with this model?

**Answer**: The problem with this model is that we have 4 spices and change in the weight of one of the species (Bream) has been repoted as intercept value, which is not correct. The model also has negative values for Persch and Roach, which is uninterpretable! Therefore, we have to set linear model with a zero intercept

```{r}
# with no intercept
lm(formula = Weight ~Species+0, data = fish)
```

Now, it is much more readable. We can say that according to our fish data, the predicted weights are 596, 372, 730, and 130 for bream, perch, pike, and roach species.

## Exercise 2

Regression lets you predict the values of a response variable from known values of explanatory variables. Which variable you use as the response variable depends on the question you are trying to answer, but in many datasets there will be an obvious choice for variables that would be interesting to predict. Over the next few exercises, you'll explore a Taiwan real estate dataset with 4 variables, which is a part of a market historical data set of real estate valuation collected from Sindian Dist., New Taipei City, Taiwan. The real estate valuation is the regression problem.

```{r}
taiwan_real_estate <- read.fst(here("data", "taiwan_real_estate.fst"))
str(taiwan_real_estate)
```

### Question 1:

TypeÂ `View(taiwan_real_estate)`Â in the console to view the dataset, and decide which variable would make a good response variable.

```{r}
View(taiwan_real_estate)
```

**Answer**: Predicting prices is a common business task, so house price makes a good response variable. So, price_twd_msq variable is one of the possible response variable for this dataset.

### Question 2:

Before you can run any statistical models, it's usually a good idea to visualize your dataset. Here, we'll look at the relationship between house price per area and the number of nearby convenience stores, using the Taiwan real estate dataset.

```{r}
plot(taiwan_real_estate$price_twd_msq, taiwan_real_estate$n_convenience)
```

ðŸ‘‹ One challenge in this dataset is that the number of convenience stores contains integer data, causing points to overlap. To solve this, you will make the points transparent. This could be best done by using ggplot

```{r}
ggplot(data = taiwan_real_estate, aes(x=n_convenience, y= price_twd_msq))+
  geom_point(alpha=0.5)+
  geom_smooth(method="lm", se=FALSE) +
  scale_x_continuous("No. of convenience stores ÅŸn walking distance")+
  scale_y_continuous("House price per unit area in Taiqan dollars per square meter")+
  ggtitle("Taiwan Real Estate Price: Conveniency effects on Prices")+
  theme_bw()
```

### Question 3:

Linear regression models always fit a straight line to the data. Straight lines are defined by two properties: their intercept and their slope.

Here, you can see a scatter plot of house price per area versus number of nearby convenience stores, using the Taiwan real estate dataset.

Also run a linear regression withÂ `price_twd_msq`Â as the response variable,Â `n_convenience`Â as the explanatory variable, andÂ `taiwan_real_estate`as the dataset.

```{r}
taiwan_model <- lm(price_twd_msq~n_convenience, data = taiwan_real_estate)
plot(price_twd_msq~n_convenience, data = taiwan_real_estate)
abline(reg = taiwan_model, col="red")
```

### 

::: callout-note
ðŸ’¡Remember that

The intercept is theÂ y-value whenÂ xÂ equals zero.

The slope is the rate of change in the y direction divided by the rate of change in the x direction.
:::

```{r}
summary(taiwan_model)
```

### Question 4:

The model had anÂ `(Intercept)`Â coefficient ofÂ `8.2242`. What does this mean?

**Answers:** On average, a house with zero convenience stores nearby had a price of 8.2242 TWD per square meter.

### Question 5:

The model had anÂ `n_convenience`Â coefficient ofÂ `0.7981`. What does this mean?

**Answers:** If you increase the number of nearby convenience stores by one, then the expected increase in house price isÂ `0.7981`Â TWD per square meter.

## Exercise 3

If the explanatory variable is categorical, the scatter plot that you used before to visualize the data doesn't make sense. Instead, a good option is to draw a barplot or histogram for each category.

The Taiwan real estate dataset has a categorical variable in the form of the age of each house. The ages have been split into 3 groups: 0 to 15 years, 15 to 30 years, and 30 to 45 years.

### Question 1:

UsingÂ `taiwan_real_estate`, plot a histogram ofÂ `price_twd_msq`Â 

```{r}
barplot(table(taiwan_real_estate$house_age_years))
```

ðŸ‘‹ A better way of visualising would be the use of ggplot faceting option

```{r}
ggplot(data = taiwan_real_estate, aes(x=price_twd_msq))+
  geom_histogram(bins = 10) +
  facet_wrap(~house_age_years)
```

### Question 2:

GroupÂ `taiwan_real_estate`Â byÂ `house_age_years`.

```{r}
table(taiwan_real_estate$house_age_years)
```

### Question 3:

Calculate the meanÂ `price_twd_msq`Â for each group

```{r}
group1 <-taiwan_real_estate[taiwan_real_estate$house_age_years=="0 to 15",]
mean(group1$price_twd_msq, na.rm=TRUE)

group2 <-taiwan_real_estate[taiwan_real_estate$house_age_years=="15 to 30",]
mean(group2$price_twd_msq, na.rm=TRUE)

group3 <-taiwan_real_estate[taiwan_real_estate$house_age_years=="30 to 45",]
mean(group3$price_twd_msq, na.rm=TRUE)
```

ðŸ‘‹ Much better and much easier way is to use dplyr group by and summarise functions

```{r}
taiwan_real_estate |> 
  group_by(house_age_years) |> 
  summarise(mean_by_group =mean(price_twd_msq))
```

### Question 4:

Run a linear regression withÂ `price_twd_msq`Â as the response variable,Â `house_age_years`Â as the explanatory variable, andÂ `taiwan_real_estate`Â as the dataset. Assign toÂ `mdl_price_vs_age`.

```{r}
mdl_price_vs_age <- lm( price_twd_msq ~ house_age_years+0, data = taiwan_real_estate)
summary(mdl_price_vs_age)
```

## Draw a conlucion

We can argue that on average house prices in Taiwan has some correlations with the age of the house. 0-15 age houses are on average cost 12.635 , while 15 to 30 years cost 9.987. Interestingly, older than 30 years houses, even more expensive that middle age houses. This requires more attention, and why these houses are expensive. One explanation could be that these older houses are relatively larger than middle age and new houses, and size could be a better estimator in estimating houses prices in Taiwan real estate market.
